This file is a merged representation of the entire codebase, combined into a single document.
Generated by Repomix on: 2025-02-09T17:16:50.079Z

================================================================
File Summary
================================================================

Purpose:
--------
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.

File Format:
------------
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Multiple file entries, each consisting of:
  a. A separator line (================)
  b. The file path (File: path/to/file)
  c. Another separator line
  d. The full contents of the file
  e. A blank line

Usage Guidelines:
-----------------
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.

Notes:
------
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded

Additional Info:
----------------

================================================================
Directory Structure
================================================================
analysis/company_extraction.py
analysis/cve_extraction.py
analysis/two_phase_grouping.py
app.py
date.py
db/database.py
docker-compose.yml
Dockerfile
llm_calls.py
main.py
pipeline.py
readme.md
requirements.txt
scrapers/bleepingcomputer.py
scrapers/darkreading-scraper.py
scrapers/krebsonsecurityscraper.py
scrapers/nist.py
scrapers/register-scraper.py
scrapers/schneier-scraper.py
scrapers/Scrapinghackernews.py
scrapers/securelist-scraper.py
scrapers/Slashdotit.py
scrapers/sophos.py
scrapers/techcrunch.py
scrapers/techradar.py
utils.py

================================================================
Files
================================================================

================
File: analysis/company_extraction.py
================
# analysis/company_extraction.py

import sqlite3
import json
import re
import time
import pandas as pd
import logging

from db.database import get_connection
from llm_calls import call_gpt_api
from utils import chunk_summaries, MAX_TOKEN_CHUNK

logger = logging.getLogger(__name__)
MODEL = "o3-mini"  # or whichever model you prefer

def get_articles_missing_company_extraction(db_path="db/news.db"):
    """
    Returns a DataFrame of articles that do NOT have any entry in article_companies.
    """
    conn = get_connection(db_path)
    query = """
        SELECT 
            a.link,
            a.title || ' - ' || a.content AS expanded_summary
        FROM articles a
        WHERE NOT EXISTS (
            SELECT 1 FROM article_companies ac
            WHERE ac.article_link = a.link
        )
        ORDER BY a.published_date DESC
    """
    df = pd.read_sql_query(query, conn)
    conn.close()
    return df

def extract_company_names_for_all_articles(api_key, db_path="db/news.db"):
    """
    Identify articles with no company extractions, parse them with LLM for company names,
    store results in article_companies.
    """
    df = get_articles_missing_company_extraction(db_path=db_path)
    if df.empty:
        logger.info("All articles already have company extractions.")
        return

    # Build a dict {link: expanded_summary}
    summaries_dict = {}
    for _, row in df.iterrows():
        link = row["link"]
        content = str(row["expanded_summary"]).strip()
        if content:
            summaries_dict[link] = content

    chunked_articles = list(chunk_summaries(summaries_dict, max_token_chunk=MAX_TOKEN_CHUNK))
    total_extractions = 0

    for idx, chunk_dict in enumerate(chunked_articles, start=1):
        logger.info(
            f"Extracting company names for chunk {idx}/{len(chunked_articles)} "
            f"with {len(chunk_dict)} articles."
        )

        prompt = (
            "You are a named-entity recognition AI. For each article, extract all company names mentioned. "
            "Return only JSON with the format:\n"
            "{ \"extractions\": [ {\"article_id\": \"...\", \"companies\": [\"CompanyA\", \"CompanyB\"]}, ... ] }\n\n"
        )
        # Append the article texts
        for art_id, text in chunk_dict.items():
            snippet = text[:5000]  # limit if needed
            prompt += f"Article ID={art_id}:\n{snippet}\n\n"

        messages = [
            {
                "role": "system",
                "content": "Extract company names from the provided article texts."
            },
            {
                "role": "user",
                "content": prompt
            }
        ]

        resp = call_gpt_api(messages, api_key, model=MODEL)
        if not resp:
            logger.warning("No response from GPT for this chunk.")
            continue

        cleaned = resp.strip().strip("```")
        cleaned = re.sub(r'^json\s+', '', cleaned, flags=re.IGNORECASE)
        try:
            data = json.loads(cleaned)
            extractions = data.get("extractions", [])
        except json.JSONDecodeError as e:
            logger.error(f"Error parsing extraction JSON: {e}\n{cleaned}")
            extractions = []

        conn = get_connection(db_path)
        c = conn.cursor()
        try:
            for item in extractions:
                article_id = item.get("article_id")
                companies = item.get("companies", [])
                if not article_id or not isinstance(companies, list):
                    continue
                for comp in companies:
                    comp_name = comp.strip()
                    if comp_name:
                        c.execute("""
                            INSERT OR IGNORE INTO article_companies (article_link, company_name)
                            VALUES (?, ?)
                        """, (article_id, comp_name))
                        total_extractions += 1
            conn.commit()
        except Exception as e:
            conn.rollback()
            logger.error(f"DB error saving company extraction: {e}")
        finally:
            conn.close()

    logger.info(
        f"Finished extracting company names. Inserted {total_extractions} new (article, company) pairs."
    )

def get_companies_in_article_list(article_links, db_path="db/news.db"):
    """
    Given a list of article links, return distinct company names from article_companies
    that match those links.
    """
    if not article_links:
        return []
    conn = get_connection(db_path)
    placeholders = ",".join("?" for _ in article_links)
    query = f"""
        SELECT DISTINCT company_name 
        FROM article_companies
        WHERE article_link IN ({placeholders})
        ORDER BY company_name ASC
    """
    rows = conn.execute(query, article_links).fetchall()
    conn.close()
    return [r[0] for r in rows]

def filter_articles_by_company(df_articles, company_name, db_path="db/news.db"):
    """
    Return only articles that mention 'company_name' in article_companies.
    If company_name is (All) or empty, return the original df_articles.
    """
    if not company_name or company_name == "(All)":
        return df_articles

    if df_articles.empty:
        return df_articles

    conn = get_connection(db_path)
    placeholders = ",".join("?" for _ in df_articles["link"])
    query = f"""
        SELECT article_link 
        FROM article_companies
        WHERE company_name = ?
          AND article_link IN ({placeholders})
    """
    params = [company_name] + list(df_articles["link"])
    matched = conn.execute(query, params).fetchall()
    conn.close()

    valid_links = {row[0] for row in matched}
    return df_articles[df_articles["link"].isin(valid_links)].copy()

================
File: analysis/cve_extraction.py
================
import requests
import json
import re
from datetime import datetime, timedelta
import pytz
import sqlite3

from db.database import (
    get_connection,
    insert_article_cve,
    insert_or_update_cve_info
)
from utils import extract_cves

#
# Regex for CVE detection
#
CVE_REGEX = r'\bCVE-\d{4}-\d{4,7}\b'

def process_cves_in_articles(db_path="db/news.db"):
    """
    - For each article in 'articles', extract CVE numbers with a simple regex.
    - Insert each CVE mention into 'article_cves' with (article_link, cve_id, published_date).
    """
    conn = get_connection(db_path)
    cursor = conn.cursor()

    # Fetch all articles
    cursor.execute("SELECT link, published_date, content FROM articles")
    articles = cursor.fetchall()
    conn.close()

    total_found = 0
    for link, published_date, content in articles:
        found_cves = extract_cves(content or "")
        if not found_cves:
            continue
        for cve in found_cves:
            insert_article_cve(link, cve, published_date, db_path=db_path)
            total_found += 1

    print(f"Finished processing CVEs in articles. Inserted {total_found} new CVE references.")

def build_cve_table(date_hours=None, db_path="db/news.db"):
    """
    Return a list or DataFrame with columns from article_cves (times seen, date range)
    plus cve_info (base score, vendor, products, etc.).
    If you use pandas, you can return a DataFrame directly.
    """
    import pandas as pd

    conn = get_connection(db_path)
    c = conn.cursor()

    # Filter by date if date_hours is provided
    if date_hours is not None:
        cutoff_utc = datetime.now(pytz.UTC) - timedelta(hours=date_hours)
        cutoff_str = cutoff_utc.isoformat()
        query = """
            SELECT ac.cve_id, ac.article_link, a.published_date
            FROM article_cves ac
            JOIN articles a ON ac.article_link = a.link
            WHERE a.published_date >= ?
        """
        rows = c.execute(query, (cutoff_str,)).fetchall()
    else:
        query = """
            SELECT ac.cve_id, ac.article_link, a.published_date
            FROM article_cves ac
            JOIN articles a ON ac.article_link = a.link
        """
        rows = c.execute(query).fetchall()

    # cve_info
    cve_info_rows = c.execute("SELECT * FROM cve_info").fetchall()
    cve_info_cols = [desc[0] for desc in c.description]
    conn.close()

    if not rows:
        empty_cols = [
            "CVE ID", "Times Seen", "First Mention", "Last Mention", "Articles",
            "Base Score", "Vendor", "Affected Products", "CVE Page Link",
            "Vendor Link", "Solution"
        ]
        return pd.DataFrame(columns=empty_cols)

    df = pd.DataFrame(rows, columns=["cve_id", "article_link", "published_date"])
    df["published_date"] = pd.to_datetime(df["published_date"], utc=True, errors="coerce")

    # Convert cve_info_rows to a dict
    cve_info_dict = {}
    for row in cve_info_rows:
        rec = dict(zip(cve_info_cols, row))
        cve_info_dict[rec["cve_id"]] = rec

    # Group by cve_id
    cve_groups = df.groupby("cve_id")
    table_rows = []
    for cve_id, group in cve_groups:
        article_links = group["article_link"].unique().tolist()
        times_seen = len(article_links)
        first_mention = group["published_date"].min()
        last_mention = group["published_date"].max()

        link_list_str = ""
        for link in article_links:
            link_list_str += f"- {link}\n"

        info = cve_info_dict.get(cve_id, {})
        base_score = info.get("base_score")
        vendor = info.get("vendor", "")
        products = info.get("affected_products", "")
        cve_page_link = info.get("cve_url", "")
        vendor_link = info.get("vendor_link", "")
        solution = info.get("solution", "")

        table_rows.append({
            "CVE ID": cve_id,
            "Times Seen": times_seen,
            "First Mention": first_mention,
            "Last Mention": last_mention,
            "Articles": link_list_str.strip(),
            "Base Score": base_score if base_score is not None else "",
            "Vendor": vendor,
            "Affected Products": products,
            "CVE Page Link": cve_page_link,
            "Vendor Link": vendor_link,
            "Solution": solution
        })

    result_df = pd.DataFrame(table_rows)
    result_df = result_df.sort_values(by=["Times Seen","CVE ID"], ascending=[False, True])
    return result_df


def update_cve_details_from_api(db_path="db/news.db"):
    """
    For each unique CVE in article_cves:
      1) Look up the CVE details via the CVE Mitre API (https://cveawg.mitre.org/api/cve/).
      2) Parse out details.
      3) Count mentions from article_cves (store as times_mentioned).
      4) Insert/update the details into the cve_info table.
    """
    conn = get_connection(db_path)
    c = conn.cursor()

    # 1) Gather all unique CVE IDs from article_cves
    c.execute("SELECT DISTINCT cve_id FROM article_cves")
    all_cves = [row[0] for row in c.fetchall()]

    # Pre-build times_mentioned from article_cves
    times_mentioned_map = {}
    rows = c.execute("""
        SELECT cve_id, COUNT(*) as cnt
        FROM article_cves
        GROUP BY cve_id
    """).fetchall()
    for r in rows:
        times_mentioned_map[r[0]] = r[1]

    updated_count = 0

    for cve_id in all_cves:
        url = f"https://cveawg.mitre.org/api/cve/{cve_id}"
        try:
            resp = requests.get(url)
            resp.raise_for_status()
            data = resp.json()
            if not isinstance(data, dict):
                continue
            if data.get("message") == "CVE not found":
                continue
        except Exception:
            continue

        is_new_format = (data.get("dataType") == "CVE_RECORD")
        base_score = None
        vendor_str = ""
        products_str = ""
        cve_page_link = f"https://cveawg.mitre.org/cve/{cve_id}"
        vendor_link = ""
        solution_str = ""

        if is_new_format:
            cna_data = data.get("containers", {}).get("cna", {})
            # (1) Attempt to find a base_score from the "metrics"
            metrics_list = []
            if isinstance(cna_data.get("metrics"), list):
                metrics_list.extend(cna_data["metrics"])
            adp_list = data.get("containers", {}).get("adp", [])
            if isinstance(adp_list, list):
                for adp_item in adp_list:
                    if isinstance(adp_item.get("metrics"), list):
                        metrics_list.extend(adp_item["metrics"])

            for m in metrics_list:
                for cvss_key in ["cvssV4_0", "cvssV3_1", "cvssV3_0", "cvssV2_0"]:
                    if cvss_key in m and isinstance(m[cvss_key], dict):
                        maybe_score = m[cvss_key].get("baseScore")
                        if maybe_score:
                            try:
                                base_score = float(maybe_score)
                                break
                            except:
                                pass
                if base_score is not None:
                    break

            # (2) Vendors / products
            affected_list = cna_data.get("affected", [])
            all_vendors = set()
            all_products = set()
            for aff in affected_list:
                v = aff.get("vendor", "")
                p = aff.get("product", "")
                if v:
                    all_vendors.add(v)
                if p:
                    all_products.add(p)
            vendor_str = ", ".join(sorted(all_vendors))
            products_str = ", ".join(sorted(all_products))

            # (3) References (try to find vendor link)
            references_list = cna_data.get("references", [])
            for ref in references_list:
                tags = ref.get("tags", [])
                url_ref = ref.get("url", "")
                if "vendor-advisory" in tags or "vendor" in url_ref.lower():
                    vendor_link = url_ref
                    break
            if not vendor_link and references_list:
                vendor_link = references_list[0].get("url", "")

            # (4) Solutions
            solutions_list = cna_data.get("solutions", [])
            if solutions_list:
                solution_texts = []
                for sol in solutions_list:
                    val = sol.get("value", "")
                    if val:
                        solution_texts.append(val)
                solution_str = "\n\n".join(solution_texts)
        else:
            # If not new format, skip or parse differently if needed
            continue

        mention_count = times_mentioned_map.get(cve_id, 0)
        raw_json_str = json.dumps(data)

        # Insert or update DB
        insert_or_update_cve_info(
            cve_id=cve_id,
            base_score=base_score,
            vendor=vendor_str,
            affected_products=products_str,
            cve_url=cve_page_link,
            vendor_link=vendor_link,
            solution=solution_str,
            times_mentioned=mention_count,
            raw_json_str=raw_json_str,
            db_path=db_path
        )
        updated_count += 1

    print(f"Updated/Inserted details for {updated_count} CVEs in cve_info table.")
    conn.close()

================
File: analysis/two_phase_grouping.py
================
# analysis/two_phase_grouping.py

import sqlite3
import json
import re
import pandas as pd
from datetime import datetime, timedelta
import pytz

from db.database import get_connection
from llm_calls import call_gpt_api
from utils import chunk_summaries, MAX_TOKEN_CHUNK

# Predefined categories, as in original code
PREDEFINED_CATEGORIES = [
    "Science & Environment",
    "Business, Finance & Trade",
    "Artificial Intelligence & Machine Learning",
    "Software Development & Open Source",
    "Cybersecurity & Data Privacy",
    "Politics & Government",
    "Consumer Technology & Gadgets",
    "Automotive, Space & Transportation",
    "Enterprise Technology & Cloud Computing",
    "Other"
]

def get_ungrouped_articles_two_phase(db_path="db/news.db"):
    """
    Articles not assigned to any two-phase category.
    """
    conn = get_connection(db_path)
    query = """
        SELECT 
            a.link as article_link,
            a.title || ' - ' || a.content as expanded_summary,
            a.published_date as created_at
        FROM articles a
        WHERE NOT EXISTS (
            SELECT 1 FROM two_phase_article_group_memberships tgm
            WHERE tgm.article_link = a.link
        )
        ORDER BY a.published_date DESC
    """
    df = pd.read_sql_query(query, conn)
    conn.close()
    return df

def get_existing_groups_two_phase(db_path="db/news.db"):
    """
    Fetch all first-level categories (two_phase_article_groups),
    along with the articles that belong to each group.
    """
    conn = get_connection(db_path)
    query = """
        SELECT 
            tpg.group_id,
            tpg.main_topic,
            tpg.sub_topic,
            tpg.group_label,
            GROUP_CONCAT(tgm.article_link) as article_links,
            COUNT(tgm.article_link) as article_count,
            tpg.created_at,
            tpg.updated_at
        FROM two_phase_article_groups tpg
        LEFT JOIN two_phase_article_group_memberships tgm 
            ON tpg.group_id = tgm.group_id
        GROUP BY tpg.group_id
        ORDER BY tpg.updated_at DESC
    """
    df = pd.read_sql_query(query, conn)
    conn.close()

    # Convert group-concatenated article_links into lists
    if not df.empty:
        df['article_links'] = df['article_links'].apply(
            lambda x: x.split(',') if x else []
        )
    return df

def get_articles_for_group_two_phase(group_id, db_path="db/news.db"):
    """
    Return all articles for a top-level two_phase group_id.
    """
    conn = get_connection(db_path)
    query = """
        SELECT 
            a.link,
            a.title,
            a.content,
            a.published_date
        FROM articles a
        JOIN two_phase_article_group_memberships tgm 
            ON a.link = tgm.article_link
        WHERE tgm.group_id = ?
        ORDER BY a.published_date DESC
    """
    df = pd.read_sql_query(query, conn, params=(group_id,))
    conn.close()
    return df

def get_articles_in_category_not_subgrouped(category: str, db_path="db/news.db"):
    """
    Return articles assigned to 'category' but NOT in any subgroups for that category.
    """
    conn = get_connection(db_path)
    query = """
        SELECT 
            a.link, 
            a.title || ' - ' || a.content AS expanded_summary, 
            a.published_date
        FROM articles a
        JOIN two_phase_article_group_memberships tgm ON tgm.article_link = a.link
        JOIN two_phase_article_groups tg ON tg.group_id = tgm.group_id
        WHERE tg.main_topic = ?
          AND NOT EXISTS (
              SELECT 1 
              FROM two_phase_subgroup_memberships tsgm
              JOIN two_phase_subgroups tsg ON tsg.subgroup_id = tsgm.subgroup_id
              WHERE tsgm.article_link = a.link
                AND tsg.category = ?
          )
        ORDER BY a.published_date DESC
    """
    df = pd.read_sql_query(query, conn, params=(category, category))
    conn.close()
    return df

def get_subgroups_for_category(category: str, db_path="db/news.db"):
    """
    Fetch subgroups in a given category from two_phase_subgroups,
    along with a count of assigned articles.
    """
    conn = get_connection(db_path)
    query = """
        SELECT 
            tsg.subgroup_id,
            tsg.category,
            tsg.group_label,
            tsg.summary,
            tsg.created_at,
            tsg.updated_at,
            COUNT(tsgm.article_link) as article_count
        FROM two_phase_subgroups tsg
        LEFT JOIN two_phase_subgroup_memberships tsgm 
            ON tsg.subgroup_id = tsgm.subgroup_id
        WHERE tsg.category = ?
        GROUP BY tsg.subgroup_id
        ORDER BY tsg.updated_at DESC
    """
    df = pd.read_sql_query(query, conn, params=(category,))
    conn.close()
    return df

def get_articles_for_subgroup(subgroup_id: int, db_path="db/news.db"):
    """
    Return articles for a given subgroup.
    """
    conn = get_connection(db_path)
    query = """
        SELECT 
            a.link, 
            a.title, 
            a.content, 
            a.published_date
        FROM articles a
        JOIN two_phase_subgroup_memberships tsgm ON a.link = tsgm.article_link
        WHERE tsgm.subgroup_id = ?
        ORDER BY a.published_date DESC
    """
    df = pd.read_sql_query(query, conn, params=(subgroup_id,))
    conn.close()
    return df

def two_phase_grouping_with_predefined_categories(summaries_dict, api_key, db_path="db/news.db"):
    """
    Assign articles to one of the PREDEFINED_CATEGORIES or 'Other'.
    Returns a dict like:
    {
      "groups": [
         {
           "main_topic": "...",
           "sub_topic": "...",
           "group_label": "...",
           "articles": [...]
         },
         ...
      ]
    }
    """
    if not summaries_dict:
        return {"groups": []}

    all_assignments = []

    # Split into large chunks so we don't exceed token limits
    for chunk_dict in chunk_summaries(summaries_dict, max_token_chunk=MAX_TOKEN_CHUNK):
        categories_text = "\n".join(f"- {cat}" for cat in PREDEFINED_CATEGORIES)
        snippet_text = ""
        for link, summary in chunk_dict.items():
            snippet_text += f"Article ID={link}:\n{summary}\n\n"

        system_msg = {
            "role": "system",
            "content": (
                "You are an AI that assigns each article to exactly one category from the list. "
                "If no category fits, choose 'Other'. Return valid JSON only."
            )
        }
        user_msg = {
            "role": "user",
            "content": (
                f"Here is the list of valid categories:\n\n{categories_text}\n\n"
                "Below are article summaries. For each article, pick one category (or 'Other'). "
                "Return JSON only, in this format:\n"
                "{ \"assignments\": [ {\"article_id\": \"...\", \"category\": \"...\"}, ... ] }\n\n"
                f"{snippet_text}"
            )
        }

        from llm_calls import call_gpt_api  # local import to avoid circular references
        response = call_gpt_api([system_msg, user_msg], api_key)
        if not response:
            continue

        cleaned = response.strip().strip("```")
        cleaned = re.sub(r'^json\s+', '', cleaned, flags=re.IGNORECASE)
        try:
            data = json.loads(cleaned)
            chunk_assignments = data.get("assignments", [])
        except Exception:
            chunk_assignments = []

        all_assignments.extend(chunk_assignments)

    grouped_data = {cat: [] for cat in PREDEFINED_CATEGORIES}
    # fallback 'Other' category
    if "Other" not in grouped_data:
        grouped_data["Other"] = []

    for assn in all_assignments:
        art_id = assn.get("article_id")
        cat = assn.get("category", "Other")
        if cat not in grouped_data:
            cat = "Other"
        if art_id:
            grouped_data[cat].append(art_id)

    result = {"groups": []}
    for cat in PREDEFINED_CATEGORIES:
        articles = grouped_data[cat]
        if articles:
            result["groups"].append({
                "main_topic": cat,
                "sub_topic": "",
                "group_label": cat,
                "articles": articles
            })
    if grouped_data["Other"]:
        result["groups"].append({
            "main_topic": "Other",
            "sub_topic": "",
            "group_label": "Other",
            "articles": grouped_data["Other"]
        })
    return result

def save_two_phase_groups(grouped_results, db_path="db/news.db"):
    """
    Save the two-phase groups (categories) to DB.
    """
    conn = get_connection(db_path)
    c = conn.cursor()
    try:
        for grp in grouped_results["groups"]:
            c.execute("""
                INSERT INTO two_phase_article_groups (main_topic, sub_topic, group_label)
                VALUES (?, ?, ?)
            """, (grp["main_topic"], grp["sub_topic"], grp["group_label"]))
            new_gid = c.lastrowid

            for art_id in grp["articles"]:
                if art_id:
                    c.execute("""
                        INSERT OR IGNORE INTO two_phase_article_group_memberships (article_link, group_id)
                        VALUES (?, ?)
                    """, (art_id, new_gid))

        conn.commit()
        print("Saved two-phase groups to DB.")
    except Exception as e:
        conn.rollback()
        print(f"Error saving two-phase groups: {e}")
    finally:
        conn.close()

def group_articles_within_category(category: str, api_key: str, db_path="db/news.db"):
    """
    Gather articles that belong to this category but have NOT been subgrouped yet,
    then cluster them by sub-topic using GPT, and insert the subgroups into DB.
    """
    df = get_articles_in_category_not_subgrouped(category, db_path=db_path)
    if df.empty:
        print(f"No un-subgrouped articles found for category '{category}'.")
        return

    summaries_dict = {}
    for _, row in df.iterrows():
        link = row["link"]
        summary = row["expanded_summary"]
        if summary:
            summaries_dict[link] = summary.strip()

    if not summaries_dict:
        print("No valid summaries for these articles.")
        return

    total_new_subgroups = 0
    chunked = list(chunk_summaries(summaries_dict, max_token_chunk=MAX_TOKEN_CHUNK))

    from llm_calls import call_gpt_api

    for i, chunk_dict in enumerate(chunked, start=1):
        print(f"Processing chunk {i}/{len(chunked)} for category: {category}")

        prompt_text = (
            "Below are articles assigned to this category. Group them by specific sub-topic.\n"
            "For each subgroup, return:\n"
            "  - group_label: a short descriptive title\n"
            "  - summary: a 2-3 sentence summary of these articles\n"
            "  - articles: an array of article IDs\n\n"
            "Return JSON only, with the structure:\n"
            "{ \"groups\": [ {\"group_label\": \"...\", \"summary\": \"...\", \"articles\": [ ... ]}, ... ] }\n\n"
        )
        for art_id, art_summary in chunk_dict.items():
            prompt_text += f"Article {art_id}: {art_summary}\n\n"

        messages = [
            {
                "role": "system",
                "content": f"You are grouping articles specifically for category '{category}'."
            },
            {
                "role": "user",
                "content": prompt_text
            }
        ]

        response = call_gpt_api(messages, api_key)
        if not response:
            print("No response from GPT for this chunk.")
            continue

        cleaned = response.strip().strip("```")
        cleaned = re.sub(r'^json\s+', '', cleaned, flags=re.IGNORECASE)
        try:
            data = json.loads(cleaned)
        except json.JSONDecodeError as e:
            print(f"Could not parse JSON for subgrouping:\n{cleaned}\nError: {e}")
            continue

        groups = data.get("groups", [])
        if not groups:
            print("No subgroups returned for this chunk.")
            continue

        conn = get_connection(db_path)
        c = conn.cursor()
        try:
            for grp in groups:
                label = grp.get("group_label", "Untitled Subgroup")
                summary = grp.get("summary", "")
                articles = grp.get("articles", [])

                c.execute("""
                    INSERT INTO two_phase_subgroups (category, group_label, summary)
                    VALUES (?, ?, ?)
                """, (category, label, summary))
                new_subgroup_id = c.lastrowid

                for art_link in articles:
                    c.execute("""
                        INSERT OR IGNORE INTO two_phase_subgroup_memberships (article_link, subgroup_id)
                        VALUES (?, ?)
                    """, (art_link, new_subgroup_id))

                total_new_subgroups += 1
            conn.commit()
            print(f"Saved {len(groups)} new subgroups for chunk {i} in category '{category}'.")
        except Exception as e:
            conn.rollback()
            print(f"Error saving subgroups: {e}")
        finally:
            conn.close()

    print(f"Done grouping articles for category '{category}'. "
          f"Total new subgroups created: {total_new_subgroups}.")

================
File: app.py
================
# app.py

import streamlit as st
import pandas as pd
import sqlite3
from datetime import datetime, timedelta
import pytz

# Import your existing modules for viewing data
from analysis.company_extraction import (
    get_companies_in_article_list,
    filter_articles_by_company
)
from analysis.cve_extraction import build_cve_table
from analysis.two_phase_grouping import (
    PREDEFINED_CATEGORIES,
    get_existing_groups_two_phase,
    get_articles_for_group_two_phase,
    get_subgroups_for_category,
    get_articles_for_subgroup
)
from db.database import setup_database

# Constants for date filtering
DATE_FILTER_OPTIONS = {
    "All time": None,
    "Last 24 hours": 24,
    "Last 7 days": 24*7,
    "Last 30 days": 24*30
}

def get_articles_for_date_range(df_articles, hours):
    if hours is None:
        return df_articles
    cutoff = datetime.now(tz=pytz.UTC) - timedelta(hours=hours)
    df_articles["published_date"] = pd.to_datetime(df_articles["published_date"], utc=True, errors="coerce")
    return df_articles.loc[df_articles["published_date"] >= cutoff].copy()

def main():
    st.title("Two-Phase Grouping Viewer: CVEs and Categories")

    # Ensure the database is set up
    setup_database()

    # -----------------------------
    # Sidebar
    # -----------------------------
    with st.sidebar:
        selected_date_range = st.selectbox("Date Filter", list(DATE_FILTER_OPTIONS.keys()))
        date_hours = DATE_FILTER_OPTIONS[selected_date_range]

        # Gather basic stats for display
        conn = sqlite3.connect("db/news.db")
        c = conn.cursor()
        
        # total articles
        c.execute("SELECT COUNT(*) FROM articles")
        total_articles = c.fetchone()[0]

        # ungrouped articles
        c.execute("""
            SELECT COUNT(*) FROM articles a
            WHERE NOT EXISTS (
                SELECT 1 FROM two_phase_article_group_memberships m
                WHERE m.article_link=a.link
            )
        """)
        ungrouped_two = c.fetchone()[0]

        # grouped
        c.execute("SELECT COUNT(*) FROM two_phase_article_group_memberships")
        grouped_two = c.fetchone()[0]

        # total groups
        c.execute("SELECT COUNT(*) FROM two_phase_article_groups")
        total_groups_two = c.fetchone()[0]

        conn.close()

        # Show overall stats in sidebar
        st.markdown("### Overall Stats")
        st.write(f"Total Articles: {total_articles}")

        st.markdown("### Two-Phase Stats")
        st.write(f"Ungrouped (awaiting category): {ungrouped_two}")
        st.write(f"Grouped (in categories): {grouped_two}")
        st.write(f"Total Two-Phase Groups: {total_groups_two}")

        # Page navigation
        st.write("---")
        pages = [
            "CVE Mentions",
            "View Two-Phase Groups"
        ]
        for cat in PREDEFINED_CATEGORIES:
            pages.append(f"Category: {cat}")
        selected_page = st.radio("Navigation", pages)

    # -----------------------------
    # Main Layout
    # -----------------------------
    col_main, col_right = st.columns([3,1], gap="large")

    # -----------------------------------
    # 1) CVE Mentions Page
    # -----------------------------------
    if selected_page == "CVE Mentions":
        with col_main:
            st.header("CVE Extraction & Mentions")

            # Build the CVE table
            cve_table = build_cve_table(date_hours, db_path="db/news.db")
            if cve_table.empty:
                st.info("No CVEs found for this date range.")
            else:
                st.write("**Current CVE Mentions**:")
                st.dataframe(cve_table, use_container_width=True)
                st.markdown("_Tip: Hover over the **Articles** cell to see links._")

        with col_right:
            st.write("Adjust date filter on the left sidebar as needed.")
        return

    # -----------------------------------
    # 2) View Two-Phase Groups Page
    # -----------------------------------
    elif selected_page == "View Two-Phase Groups":
        with col_main:
            st.header("View Two-Phase Groups")
            st.write(f"**Date Filter:** {selected_date_range}")

            df2 = get_existing_groups_two_phase(db_path="db/news.db")
            if df2.empty:
                st.info("No two-phase groups found.")
            else:
                valid_groups = []
                for _, row in df2.iterrows():
                    group_id = row["group_id"]
                    articles_df = get_articles_for_group_two_phase(group_id, db_path="db/news.db")
                    articles_df = get_articles_for_date_range(articles_df, date_hours)
                    if not articles_df.empty:
                        valid_groups.append(row)

                if not valid_groups:
                    st.warning("No groups found under this date filter.")
                else:
                    filtered2 = pd.DataFrame(valid_groups)
                    new_counts = []
                    for _, r in filtered2.iterrows():
                        g_id = r["group_id"]
                        arts = get_articles_for_group_two_phase(g_id, db_path="db/news.db")
                        arts = get_articles_for_date_range(arts, date_hours)
                        new_counts.append(len(arts))
                    filtered2["article_count"] = new_counts

                    sort_by_size_2 = st.checkbox("Sort by largest group size?")
                    if sort_by_size_2:
                        filtered2 = filtered2.sort_values(by="article_count", ascending=False)

                    st.write(f"Showing {len(filtered2)} groups after date filter.")
                    st.dataframe(filtered2[["group_id","main_topic","sub_topic","group_label","article_count"]])

                    # Let user choose a group to expand
                    group_ids_2 = filtered2["group_id"].tolist()
                    if group_ids_2:
                        chosen_2 = st.selectbox("Select a group to view details", group_ids_2, key="chosen_2_val")
                        if chosen_2:
                            st.subheader(f"Group {chosen_2}")
                            articles_2p = get_articles_for_group_two_phase(chosen_2, db_path="db/news.db")
                            articles_2p = get_articles_for_date_range(articles_2p, date_hours)
                            if articles_2p.empty:
                                st.warning("No articles found in this date filter.")
                            else:
                                for _, row in articles_2p.iterrows():
                                    with st.expander(f"{row['title']} ({row['published_date']})"):
                                        st.write(row["content"])
                                        st.write(f"Link: {row['link']}")
        return

    # -----------------------------------
    # 3) Category-Specific Pages
    # -----------------------------------
    else:
        # The user selected "Category: X"
        category_name = selected_page.replace("Category: ", "")
        with col_main:
            st.header(f"Fine-Grained Subgroups in Category: {category_name}")
            st.write(f"**Date Filter:** {selected_date_range}")

            sub_df = get_subgroups_for_category(category_name, db_path="db/news.db")
            if sub_df.empty:
                st.info("No subgroups found. Possibly run the pipeline on the backend.")
            else:
                valid_subgroups = []
                for _, srow in sub_df.iterrows():
                    sg_id = srow["subgroup_id"]
                    arts_df = get_articles_for_subgroup(sg_id, db_path="db/news.db")
                    arts_df = get_articles_for_date_range(arts_df, date_hours)
                    if not arts_df.empty:
                        new_row = dict(srow)
                        new_row["filtered_article_count"] = len(arts_df)
                        valid_subgroups.append(new_row)
                if not valid_subgroups:
                    st.warning("No subgroups match this date filter.")
                else:
                    sub_filtered = pd.DataFrame(valid_subgroups)
                    sub_filtered = sub_filtered.sort_values(by="filtered_article_count", ascending=False)
                    for _, row in sub_filtered.iterrows():
                        subgroup_id = row["subgroup_id"]
                        group_label = row["group_label"]
                        summary = row.get("summary") or "(No summary)"
                        article_count = row["filtered_article_count"]

                        with st.expander(f"{group_label} (Articles: {article_count})"):
                            st.write(summary)
                            arts_in_subgroup = get_articles_for_subgroup(subgroup_id, db_path="db/news.db")
                            arts_in_subgroup = get_articles_for_date_range(arts_in_subgroup, date_hours)
                            for _, arow in arts_in_subgroup.iterrows():
                                st.write(f"- **{arow['title']}** ({arow['published_date']})")
                                st.caption(f"[Link]({arow['link']})")

if __name__ == "__main__":
    main()

================
File: date.py
================
#!/usr/bin/env python3
import sqlite3
from dateutil import parser
import datetime

def convert_dates(conn):
    cur = conn.cursor()
    cur.execute("SELECT link, published_date FROM articles")
    rows = cur.fetchall()

    for row in rows:
        link, published_date = row
        if not published_date:
            continue
        try:
            dt = parser.parse(published_date)
            if dt.tzinfo is None:
                dt = dt.replace(tzinfo=datetime.timezone.utc)
            dt_utc = dt.astimezone(datetime.timezone.utc)
            new_date = dt_utc.strftime("%Y-%m-%dT%H:%M:%SZ")
        except Exception as e:
            print(f"Error parsing date '{published_date}' for link '{link}': {e}")
            continue

        cur.execute("UPDATE articles SET published_date = ? WHERE link = ?", (new_date, link))
        print(f"Updated article (link: {link}): '{published_date}' -> '{new_date}'")

    conn.commit()

def main():
    conn = sqlite3.connect("db/news.db")
    try:
        convert_dates(conn)
    finally:
        conn.close()

if __name__ == '__main__':
    main()

================
File: db/database.py
================
import sqlite3
import time
from datetime import datetime

def get_connection(db_path="db/news.db"):
    """
    Returns a new connection to the SQLite database.
    """
    return sqlite3.connect(db_path)

def setup_database(db_path="db/news.db"):
    """
    Create all necessary tables in the SQLite database.
    (Call this once at startup or whenever you need to ensure the schema exists.)
    """
    conn = get_connection(db_path)
    cursor = conn.cursor()

    # Articles table (assumes you have this table in your schema)
    # Adjust as needed if you store articles differently
    cursor.execute("""
    CREATE TABLE IF NOT EXISTS articles (
        link TEXT PRIMARY KEY,
        title TEXT,
        content TEXT,
        published_date TIMESTAMP
    )
    """)

    # -------------------------------
    # Two-phase grouping tables
    # -------------------------------
    cursor.execute("""
    CREATE TABLE IF NOT EXISTS two_phase_article_groups (
        group_id INTEGER PRIMARY KEY AUTOINCREMENT,
        main_topic TEXT NOT NULL,
        sub_topic TEXT NOT NULL,
        group_label TEXT NOT NULL,
        created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
        updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
    )
    """)
    cursor.execute("""
    CREATE TABLE IF NOT EXISTS two_phase_article_group_memberships (
        article_link TEXT NOT NULL,
        group_id INTEGER NOT NULL,
        added_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
        FOREIGN KEY (group_id) REFERENCES two_phase_article_groups (group_id),
        PRIMARY KEY (article_link, group_id)
    )
    """)

    # -------------------------------
    # Subgroup tables
    # -------------------------------
    cursor.execute("""
    CREATE TABLE IF NOT EXISTS two_phase_subgroups (
        subgroup_id INTEGER PRIMARY KEY AUTOINCREMENT,
        category TEXT NOT NULL,
        group_label TEXT NOT NULL,
        summary TEXT,
        created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
        updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
    )
    """)
    cursor.execute("""
    CREATE TABLE IF NOT EXISTS two_phase_subgroup_memberships (
        article_link TEXT NOT NULL,
        subgroup_id INTEGER NOT NULL,
        added_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
        FOREIGN KEY (subgroup_id) REFERENCES two_phase_subgroups (subgroup_id),
        PRIMARY KEY (article_link, subgroup_id)
    )
    """)

    # -------------------------------
    # Company references
    # -------------------------------
    cursor.execute("""
    CREATE TABLE IF NOT EXISTS article_companies (
        article_link TEXT NOT NULL,
        company_name TEXT NOT NULL,
        PRIMARY KEY(article_link, company_name)
    )
    """)

    # -------------------------------
    # CVE references + CVE info
    # -------------------------------
    cursor.execute("""
    CREATE TABLE IF NOT EXISTS article_cves (
        article_link TEXT NOT NULL,
        cve_id TEXT NOT NULL,
        published_date TIMESTAMP,
        PRIMARY KEY (article_link, cve_id)
    )
    """)
    cursor.execute("""
    CREATE TABLE IF NOT EXISTS cve_info (
        cve_id TEXT PRIMARY KEY,
        base_score REAL,
        vendor TEXT,
        affected_products TEXT,
        cve_url TEXT,
        vendor_link TEXT,
        solution TEXT,
        times_mentioned INTEGER DEFAULT 0,
        raw_json TEXT,
        created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
        updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
    )
    """)

    conn.commit()
    conn.close()

#
# Below you can add optional "getter" or "setter" functions that encapsulate queries
# for articles, groups, etc. For example:
#

def insert_article_company(article_link, company_name, db_path="db/news.db"):
    """
    Insert or ignore a (article_link, company_name) pair into article_companies.
    """
    conn = get_connection(db_path)
    cur = conn.cursor()
    try:
        cur.execute("""
            INSERT OR IGNORE INTO article_companies (article_link, company_name)
            VALUES (?, ?)
        """, (article_link, company_name))
        conn.commit()
    finally:
        conn.close()

def insert_article_cve(article_link, cve_id, published_date, db_path="db/news.db"):
    """
    Insert or ignore a (article_link, cve_id, published_date) record into article_cves.
    """
    conn = get_connection(db_path)
    cur = conn.cursor()
    try:
        cur.execute("""
            INSERT OR IGNORE INTO article_cves (article_link, cve_id, published_date)
            VALUES (?, ?, ?)
        """, (article_link, cve_id, published_date))
        conn.commit()
    finally:
        conn.close()

def insert_or_update_cve_info(cve_id,
                              base_score,
                              vendor,
                              affected_products,
                              cve_url,
                              vendor_link,
                              solution,
                              times_mentioned,
                              raw_json_str,
                              db_path="db/news.db"):
    """
    Insert or update the cve_info table with the given details.
    """
    conn = get_connection(db_path)
    cur = conn.cursor()
    try:
        cur.execute("""
            INSERT INTO cve_info (
                cve_id,
                base_score,
                vendor,
                affected_products,
                cve_url,
                vendor_link,
                solution,
                times_mentioned,
                raw_json
            )
            VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)
            ON CONFLICT(cve_id) DO UPDATE SET
                base_score=excluded.base_score,
                vendor=excluded.vendor,
                affected_products=excluded.affected_products,
                cve_url=excluded.cve_url,
                vendor_link=excluded.vendor_link,
                solution=excluded.solution,
                times_mentioned=excluded.times_mentioned,
                raw_json=excluded.raw_json,
                updated_at=CURRENT_TIMESTAMP
        """, (
            cve_id,
            base_score,
            vendor,
            affected_products,
            cve_url,
            vendor_link,
            solution,
            times_mentioned,
            raw_json_str
        ))
        conn.commit()
    finally:
        conn.close()

# Add more DB helper functions here if needed...

================
File: docker-compose.yml
================
services:
  app:
    build: .
    container_name: my_scraper_app
    ports:
      - "8501:8501"
    volumes:
      - .:/app

================
File: Dockerfile
================
# Dockerfile
FROM python:3.9-slim

WORKDIR /app

COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# Copy the entire codebase into the container
COPY . /app

# Expose Streamlit port
EXPOSE 8501

# Default command to run the 'main.py' script
# which eventually calls "streamlit run app.py"
CMD ["python", "main.py"]

================
File: llm_calls.py
================
# llm_calls.py

import os
import time
import logging
from openai import OpenAI

MODEL = "o3-mini"
MAX_RETRIES = 3
REQUEST_TIMEOUT = 240

logger = logging.getLogger(__name__)

def call_gpt_api(messages, api_key=None, model=MODEL):
    """
    Call OpenAI API with retry logic and basic error handling.
    If api_key is not provided, attempts to get it from environment variables.
    """
    if api_key is None:
        api_key = os.getenv("OPENAI_API_KEY")
        if not api_key:
            logger.error("No API key provided. Please set OPENAI_API_KEY environment variable.")
            return None

    # Estimate tokens (very rough)
    total_token_estimate = int(sum(len(m["content"].split()) for m in messages) * 1.3)
    logger.info("API Request Details:")
    logger.info(f"- Model: {model}")
    logger.info(f"- Timeout: {REQUEST_TIMEOUT}s")
    logger.info(f"- Message count: {len(messages)}")
    logger.info(f"- Approx token count: {total_token_estimate}")

    client = OpenAI(api_key=api_key)
    for attempt in range(MAX_RETRIES):
        try:
            logger.info(f"Making API call (attempt {attempt+1}/{MAX_RETRIES}) to {model}...")
            start_time = time.time()
            response = client.chat.completions.create(
                model=model,
                messages=messages,
                timeout=REQUEST_TIMEOUT
            )
            elapsed_time = time.time() - start_time
            logger.info(f"API call successful in {elapsed_time:.2f}s with model='{model}'")
            return response.choices[0].message.content.strip()

        except Exception as e:
            elapsed_time = time.time() - start_time
            logger.error(f"Error on attempt {attempt+1}: {type(e).__name__}: {e}")
            if attempt < MAX_RETRIES - 1:
                logger.warning("Retrying in 2 seconds...")
                time.sleep(2)
            else:
                return None

================
File: main.py
================
# main.py
import subprocess
import sys
import threading
import os

# Import the pipeline function from pipeline.py
from pipeline import run_full_pipeline_headless

import os
api_key = os.getenv('OPENAI_API_KEY')
if not api_key:
    print("Error: OPENAI_API_KEY environment variable not found")
    sys.exit(1)
logs = run_full_pipeline_headless(api_key=api_key, db_path="db/news.db")

def run_scraper(script_path: str):
    print(f"--- Running {script_path} ---")
    subprocess.run([sys.executable, script_path], check=True)

def run_all_scrapers_in_threads(scraper_scripts):
    threads = []

    def wrapper(script):
        try:
            run_scraper(script)
        except subprocess.CalledProcessError as e:
            print(f"Error running {script}: {e}")

    for script in scraper_scripts:
        t = threading.Thread(target=wrapper, args=(script,))
        t.start()
        threads.append(t)

    for t in threads:
        t.join()

def main():
    # 1) Define the scraper scripts
    scraper_scripts = [
        "scrapers/bleepingcomputer.py",
        "scrapers/darkreading-scraper.py",
        "scrapers/krebsonsecurityscraper.py",
        "scrapers/nist.py",
        "scrapers/register-scraper.py",
        "scrapers/schneier-scraper.py",
        "scrapers/Scrapinghackernews.py",
        "scrapers/securelist-scraper.py",
        "scrapers/Slashdotit.py",
        "scrapers/sophos.py",
        "scrapers/techcrunch.py",
        "scrapers/techradar.py",
    ]

    # 2) Run all scrapers
    run_all_scrapers_in_threads(scraper_scripts)

    # 3) Run date.py to standardize publication dates
    print("\n--- Running date.py to standardize publication dates ---")
    try:
        run_scraper("date.py")
    except subprocess.CalledProcessError as e:
        print(f"Error running date.py: {e}")

    # 4) Automatically run the pipeline (assuming your pipeline doesnt need an API key now)
    print("\n--- Running the full pipeline (headless) ---")
    logs = run_full_pipeline_headless(db_path="db/news.db")
    for line in logs:
        print(line)
    print("--- Finished pipeline ---")

    # 5) Launch Streamlit UI for browsing
    print("\n--- Starting Streamlit interface ---")
    try:
        subprocess.run(["streamlit", "run", "app.py"], check=True)
    except subprocess.CalledProcessError as e:
        print(f"Error launching Streamlit app: {e}")

if __name__ == "__main__":
    main()

================
File: pipeline.py
================
# -------------------------------------------------------------
# In app.py (or pipeline.py if you prefer a separate file)
# -------------------------------------------------------------

from analysis.company_extraction import extract_company_names_for_all_articles
from analysis.cve_extraction import process_cves_in_articles, update_cve_details_from_api
from analysis.two_phase_grouping import (
    get_ungrouped_articles_two_phase,
    two_phase_grouping_with_predefined_categories,
    save_two_phase_groups,
    group_articles_within_category,
    PREDEFINED_CATEGORIES
)

def run_full_pipeline_headless(api_key=None, db_path="db/news.db"):
    """
    Run all steps in one go, but WITHOUT any Streamlit calls.
    If api_key is not provided, attempts to get it from environment variable.
    Returns a dict of messages or logs that you can print or ignore.
    """
    import os
    if api_key is None:
        api_key = os.getenv('OPENAI_API_KEY')
        if not api_key:
            return ["Error: No API key provided and OPENAI_API_KEY environment variable not found"]

    logs = []

    # 1) Extract company names
    logs.append("Extracting company names...")
    extract_company_names_for_all_articles(api_key, db_path=db_path)
    logs.append("Done extracting company names.")

    # 2) Extract CVE mentions
    logs.append("Extracting CVE mentions from articles...")
    process_cves_in_articles(db_path=db_path)
    logs.append("Done extracting CVE mentions.")

    # 3) Pull CVE details from MITRE
    logs.append("Pulling CVE details from MITRE API...")
    update_cve_details_from_api(db_path=db_path)
    logs.append("Done pulling CVE details.")

    # 4) Group ungrouped articles into top-level categories
    df = get_ungrouped_articles_two_phase(db_path=db_path)
    if df.empty:
        logs.append("No ungrouped articles found for top-level grouping.")
    else:
        logs.append(f"Found {len(df)} articles needing top-level grouping.")
        summaries_dict = {}
        for _, row in df.iterrows():
            s = str(row["expanded_summary"]).strip()
            if s:
                summaries_dict[row["article_link"]] = s
        if summaries_dict:
            result = two_phase_grouping_with_predefined_categories(summaries_dict, api_key, db_path=db_path)
            if result["groups"]:
                save_two_phase_groups(result, db_path=db_path)
                logs.append("Saved top-level groups.")
            else:
                logs.append("No groups created in two-phase approach.")
        else:
            logs.append("No valid summaries for top-level grouping.")

    # 5) Sub-group articles for each predefined category
    logs.append("Sub-grouping for each predefined category...")
    for cat in PREDEFINED_CATEGORIES:
        group_articles_within_category(cat, api_key, db_path=db_path)
        logs.append(f"Finished grouping articles for category: {cat}")

    logs.append("All steps in the pipeline are complete.")
    return logs

================
File: readme.md
================
# Cybersecurity News Aggregator

A comprehensive cybersecurity news aggregation system that scrapes multiple authoritative sources, stores articles in a SQLite database, and provides an interactive Streamlit interface for viewing and analyzing the content.

## Features

- Multi-source news scraping from reputable cybersecurity websites:
  - BleepingComputer
  - Dark Reading
  - Krebs on Security
  - NIST Cybersecurity
  - The Register
  - Schneier on Security
  - The Hacker News
  - Securelist
  - Slashdot
  - Sophos
  - TechCrunch
  - TechRadar

- Automated article grouping and analysis using OpenAI's API
- Interactive Streamlit dashboard for viewing and managing content
- Docker support for easy deployment
- Automatic date standardization
- Duplicate detection and filtering
- Rate limiting and respectful scraping practices

## Prerequisites

- Docker and Docker Compose
- OpenAI API key (for article grouping features)

## Quick Start

1. Clone the repository:
```bash
git clone [repository-url]
cd [repository-name]
```

2. Build and run using Docker Compose:
```bash
docker-compose up --build
```

The application will be available at `http://localhost:8501`

## Configuration

- OpenAI API key can be entered directly in the Streamlit interface

## Architecture

The system consists of several components:

1. **Scrapers**: Individual Python scripts for each news source
2. **Database**: SQLite database for storing articles and their metadata
3. **Date Standardization**: Ensures consistent datetime formats
4. **Article Grouping**: Two approaches:
   - Single-step grouping
   - Two-phase grouping
5. **Web Interface**: Streamlit dashboard for interaction and visualization

## System Flow

1. Scrapers collect articles from various sources
2. Articles are stored in the SQLite database
3. Date standardization is performed
4. Articles can be grouped using AI-powered analysis
5. Content is viewable and manageable through the Streamlit interface

## Project Structure

```
.
 Dockerfile
 docker-compose.yml
 requirements.txt
 main.py
 date.py
 streamlit_app.py
 run_scrapers.py
 scrapers/
     bleepingcomputer.py
     darkreading-scraper.py
     krebsonsecurityscraper.py
     nist.py
     register-scraper.py
     schneier-scraper.py
     Scrapinghackernews.py
     securelist-scraper.py
     Slashdotit.py
     sophos.py
     techcrunch.py
     techradar.py
```

## Manual Setup (Without Docker)

If you prefer to run without Docker:

1. Create a Python virtual environment:
```bash
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
```

2. Install dependencies:
```bash
pip install -r requirements.txt
```

3. Run the application:
```bash
python main.py
```

================
File: requirements.txt
================
beautifulsoup4
feedparser
requests
urllib3
streamlit
pandas
openai

================
File: scrapers/bleepingcomputer.py
================
import sqlite3
import requests
from bs4 import BeautifulSoup
import feedparser
import time
import sys
from typing import Optional, Dict, Any, List

class BleepingComputerScraper:
    def __init__(self, 
                 db_name: str = 'db/news.db', 
                 feed_url: str = "https://www.bleepingcomputer.com/feed/"):
        """
        Initialize the BleepingComputer scraper
        db_name  : SQLite database file name
        feed_url : BleepingComputer RSS feed URL
        """
        self.db_name = db_name
        self.feed_url = feed_url
        # Using a session for faster repeated requests
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': ('Mozilla/5.0 (Windows NT 10.0; Win64; x64) '
                           'AppleWebKit/537.36 (KHTML, like Gecko) '
                           'Chrome/91.0.4472.124 Safari/537.36')
        })
        self.setup_database()

    def setup_database(self):
        """Initialize the database with an articles table to store scraped content."""
        try:
            with sqlite3.connect(self.db_name) as conn:
                c = conn.cursor()
                c.execute("""
                    CREATE TABLE IF NOT EXISTS articles (
                        link TEXT PRIMARY KEY,
                        title TEXT NOT NULL,
                        published_date TIMESTAMP,
                        content TEXT NOT NULL,
                        source TEXT NOT NULL,
                        processed_date TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                    )
                """)
                conn.commit()
        except sqlite3.Error as e:
            sys.exit(f"Database initialization error: {e}")

    def fetch_feed_entries(self) -> List[Dict[str, Any]]:
        """Fetch the BleepingComputer RSS feed and return a list of feed entries."""
        try:
            feed = feedparser.parse(self.feed_url)
            entries = []
            for entry in feed.entries:
                entries.append({
                    'link': entry.link,
                    'title': entry.title,
                    'published_date': getattr(entry, 'published', None)
                })
            return entries
        except Exception as e:
            print(f"Error fetching feed entries: {e}")
            return []

    def scrape_article(self, url: str) -> Optional[str]:
        """Scrape the full article content from a BleepingComputer article."""
        try:
            response = self.session.get(url, timeout=10)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # Find the article body
            article_body = soup.find('div', class_='articleBody')
            if not article_body:
                return None

            # Remove related articles section
            related_articles = article_body.find('div', class_='cz-related-article-wrapp')
            if related_articles:
                related_articles.decompose()

            # Extract text
            paragraphs = article_body.find_all('p')
            article_text = '\n\n'.join(
                p.get_text().strip() for p in paragraphs if p.get_text().strip()
            )
            
            return article_text if article_text else None

        except requests.RequestException as e:
            print(f"Request error while scraping {url}: {e}")
            return None
        except Exception as e:
            print(f"Error processing {url}: {e}")
            return None

    def already_processed(self, link: str) -> bool:
        """Check if this link is already stored in the database."""
        try:
            with sqlite3.connect(self.db_name) as conn:
                c = conn.cursor()
                c.execute("SELECT link FROM articles WHERE link = ?", (link,))
                row = c.fetchone()
                return bool(row)
        except sqlite3.Error:
            return False

    def process_articles(self, limit: int = 100):
        """Main processing function to fetch and store articles."""
        feed_entries = self.fetch_feed_entries()
        if not feed_entries:
            print("No feed entries found.")
            return

        # Filter for new entries
        new_entries = []
        for entry in feed_entries:
            if not self.already_processed(entry['link']):
                new_entries.append(entry)
            if len(new_entries) >= limit:
                break

        if not new_entries:
            print("No new articles to process.")
            return

        for entry in new_entries:
            print(f"\nProcessing article: {entry['title']}")
            content = self.scrape_article(entry['link'])
            if not content:
                print(f"Failed to scrape content for {entry['link']}\n")
                continue

            try:
                with sqlite3.connect(self.db_name) as conn:
                    c = conn.cursor()
                    c.execute("""
                        INSERT OR REPLACE INTO articles 
                        (link, title, published_date, content, source)
                        VALUES (?, ?, ?, ?, ?)
                    """, (
                        entry['link'],
                        entry['title'],
                        entry['published_date'],
                        content,
                        "bleepingcomputer"
                    ))
                    conn.commit()
                
                print(f"Title: {entry['title']}")
                print(f"Link: {entry['link']}")
                print(f"Published: {entry['published_date']}")
                print("\nContent Preview:")
                print(content[:500] + "...\n")
                print("-" * 80 + "\n")

            except sqlite3.Error as db_error:
                print(f"Database error while storing article: {db_error}")
                continue

            time.sleep(2)  # Rate limiting

def main():
    scraper = BleepingComputerScraper(
        db_name='db/news.db',
        feed_url='https://www.bleepingcomputer.com/feed/'
    )
    scraper.process_articles(limit=100)

if __name__ == "__main__":
    main()

================
File: scrapers/darkreading-scraper.py
================
import sqlite3
import requests
from bs4 import BeautifulSoup
import time
from datetime import datetime
import re
import xml.etree.ElementTree as ET
from email.utils import parsedate_to_datetime
from typing import Optional, List, Dict, Any
from difflib import SequenceMatcher

class DarkReadingScraper:
    def __init__(self, db_name: str = 'db/news.db', site_config: Dict[str, Any] = None):
        self.db_name = db_name
        self.site_config = site_config or {}
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': ('Mozilla/5.0 (Windows NT 10.0; Win64; x64) '
                           'AppleWebKit/537.36 (KHTML, like Gecko) '
                           'Chrome/91.0.4472.124 Safari/537.36')
        })
        self.setup_database()

    def setup_database(self):
        with sqlite3.connect(self.db_name) as conn:
            c = conn.cursor()
            c.execute("""
                CREATE TABLE IF NOT EXISTS articles (
                    link TEXT PRIMARY KEY,
                    title TEXT NOT NULL,
                    published_date TIMESTAMP,
                    content TEXT,
                    source TEXT NOT NULL,
                    processed_date TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                )
            """)
            conn.commit()

    def clean_text(self, text: str) -> str:
        if not text:
            return ""
        text = self.remove_emojis(text)
        return ' '.join(text.lower().split())

    def is_similar_content(self, text1: str, text2: str, threshold: float = 0.85) -> bool:
        return SequenceMatcher(None, self.clean_text(text1), self.clean_text(text2)).ratio() > threshold

    def is_duplicate(self, link: str, title: str, content: str) -> bool:
        try:
            with sqlite3.connect(self.db_name) as conn:
                c = conn.cursor()
                
                c.execute("SELECT link FROM articles WHERE link = ?", (link,))
                if c.fetchone():
                    return True

                c.execute("SELECT title, content FROM articles WHERE source = 'darkreading'")
                existing_articles = c.fetchall()
                
                new_title_clean = self.clean_text(title)
                new_content_clean = self.clean_text(content)
                
                for existing_title, existing_content in existing_articles:
                    if self.is_similar_content(new_title_clean, existing_title, 0.9):
                        if self.is_similar_content(new_content_clean, existing_content, 0.85):
                            return True
                return False
                
        except sqlite3.Error:
            return False

    def remove_emojis(self, text: str) -> str:
        emoji_pattern = re.compile(
            "["
            "\U0001F600-\U0001F64F"
            "\U0001F300-\U0001F5FF"
            "\U0001F680-\U0001F6FF"
            "\U0001F1E0-\U0001F1FF"
            "\U00002702-\U000027B0"
            "\U000024C2-\U0001F251"
            "]+",
            flags=re.UNICODE
        )
        return emoji_pattern.sub(r"", text)

    def parse_rss_feed(self, feed_content: str) -> List[Dict[str, Any]]:
        try:
            root = ET.fromstring(feed_content)
            channel = root.find('channel')
            if channel is None:
                return []
            articles = []
            for item in channel.findall('item'):
                title_elem = item.find('title')
                link_elem = item.find('link')
                pub_date_elem = item.find('pubDate')
                description_elem = item.find('description')

                title = title_elem.text if title_elem is not None else None
                link = link_elem.text if link_elem is not None else None
                pub_date = pub_date_elem.text if pub_date_elem is not None else None
                description = description_elem.text if description_elem is not None else ''

                articles.append({
                    'link': link,
                    'title': title,
                    'published_date': pub_date,
                    'description': description
                })
            return articles
        except (ET.ParseError, Exception):
            return []

    def scrape_article(self, url: str) -> Optional[str]:
        try:
            response = self.session.get(url, timeout=10)
            response.raise_for_status()

            soup = BeautifulSoup(response.content, 'html.parser')
            article_div = soup.find('div', class_='ArticleBase-BodyContent')
            if not article_div:
                return None

            for tag in article_div.find_all(['div', 'p'], class_=['RelatedArticle', 'ContentImage-Link']):
                tag.decompose()

            paragraphs = article_div.find_all('p', class_='ContentParagraph')
            article_text = '\n'.join(p.get_text().strip() for p in paragraphs if p.get_text().strip())

            headers_tags = article_div.find_all(['h1', 'h2', 'h3'])
            for header in headers_tags:
                header_text = header.get_text().strip()
                if header_text:
                    article_text = header_text + "\n\n" + article_text

            return article_text

        except (requests.RequestException, Exception):
            return None

    def process_feed(self, feed_url: str, source_name: str):
        try:
            response = self.session.get(feed_url, timeout=10)
            response.raise_for_status()
            feed_content = response.text

            articles = self.parse_rss_feed(feed_content)
            seen_links = set()

            for article in articles:
                if not article.get('link'):
                    continue
                if article['link'] in seen_links:
                    continue
                seen_links.add(article['link'])

                if not all(k in article for k in ['link', 'title', 'published_date']):
                    continue

                cleaned_title = self.remove_emojis(article['title'])
                content = self.scrape_article(article['link'])
                
                if not content:
                    continue

                if self.is_duplicate(article['link'], cleaned_title, content):
                    continue

                pub_date = None
                if article['published_date']:
                    try:
                        pub_date = parsedate_to_datetime(article['published_date'])
                    except Exception:
                        pub_date = article['published_date']

                try:
                    with sqlite3.connect(self.db_name) as conn:
                        c = conn.cursor()
                        c.execute("""
                            INSERT OR REPLACE INTO articles 
                            (link, title, published_date, content, source)
                            VALUES (?, ?, ?, ?, ?)
                        """, (
                            article['link'],
                            cleaned_title,
                            pub_date,
                            content,
                            source_name
                        ))
                        conn.commit()

                    print(f"Link: {article['link']}")
                    print(f"Published: {article['published_date']}")
                    print(f"**{cleaned_title}**")
                    print(content.strip())
                    print("\n---\n")

                    time.sleep(1)  # Rate limit
                except sqlite3.Error:
                    continue

        except (requests.RequestException, Exception):
            pass

def main():
    site_config = {
        "darkreading": {
            "feed_url": "https://www.darkreading.com/rss.xml"
        }
    }
    scraper = DarkReadingScraper(site_config=site_config)
    feed_url = site_config["darkreading"]["feed_url"]
    scraper.process_feed(feed_url, "darkreading")

if __name__ == "__main__":
    main()

================
File: scrapers/krebsonsecurityscraper.py
================
import sqlite3
import requests
from bs4 import BeautifulSoup
import feedparser
import time
import sys
from typing import Optional, Dict, Any, List
from difflib import SequenceMatcher

class KrebsScraper:
    def __init__(self, 
                 db_name: str = 'db/news.db', 
                 feed_url: str = "https://krebsonsecurity.com/feed/"):
        self.db_name = db_name
        self.feed_url = feed_url
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': ('Mozilla/5.0 (Windows NT 10.0; Win64; x64) '
                           'AppleWebKit/537.36 (KHTML, like Gecko) '
                           'Chrome/91.0.4472.124 Safari/537.36')
        })
        self.setup_database()

    def setup_database(self):
        try:
            with sqlite3.connect(self.db_name) as conn:
                c = conn.cursor()
                c.execute("""
                    CREATE TABLE IF NOT EXISTS articles (
                        link TEXT PRIMARY KEY,
                        title TEXT NOT NULL,
                        published_date TIMESTAMP,
                        content TEXT NOT NULL,
                        source TEXT NOT NULL,
                        processed_date TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                    )
                """)
                conn.commit()
        except sqlite3.Error as e:
            sys.exit(f"Database initialization error: {e}")

    def clean_text(self, text: str) -> str:
        if not text:
            return ""
        return ' '.join(text.lower().split())

    def is_similar_content(self, text1: str, text2: str, threshold: float = 0.85) -> bool:
        return SequenceMatcher(None, self.clean_text(text1), self.clean_text(text2)).ratio() > threshold

    def is_duplicate(self, link: str, title: str, content: str) -> bool:
        try:
            with sqlite3.connect(self.db_name) as conn:
                c = conn.cursor()
                c.execute("SELECT link FROM articles WHERE link = ?", (link,))
                if c.fetchone():
                    print(f"Duplicate found (exact link match): {link}")
                    return True

                c.execute("SELECT title, content FROM articles WHERE source = 'krebs'")
                existing_articles = c.fetchall()
                
                clean_title = self.clean_text(title)
                clean_content = self.clean_text(content)
                
                for existing_title, existing_content in existing_articles:
                    if self.is_similar_content(clean_title, existing_title, 0.9):
                        if self.is_similar_content(clean_content, existing_content, 0.85):
                            print(f"Duplicate found (similar content): {link}")
                            return True
                return False
        except sqlite3.Error as e:
            print(f"Database error while checking duplicates: {e}")
            return False

    def fetch_krebs_feed_entries(self) -> List[Dict[str, Any]]:
        try:
            feed = feedparser.parse(self.feed_url)
            entries = []
            for entry in feed.entries:
                link = getattr(entry, 'link', None)
                title = getattr(entry, 'title', "No Title")
                published = getattr(entry, 'published', None)
                if link:
                    entries.append({
                        'link': link,
                        'title': title,
                        'published_date': published
                    })
            return entries
        except Exception as e:
            print(f"Error fetching feed entries: {e}")
            return []

    def scrape_article(self, url: str) -> Optional[str]:
        try:
            response = self.session.get(url, timeout=10)
            response.raise_for_status()
            soup = BeautifulSoup(response.content, 'html.parser')
            content_div = soup.find('div', class_='entry-content')
            if not content_div:
                return None

            paragraphs = content_div.find_all('p')
            article_text = '\n'.join(p.get_text().strip() for p in paragraphs if p.get_text().strip())
            return article_text if article_text else None
        except requests.RequestException as e:
            print(f"Request error while scraping {url}: {e}")
            return None
        except Exception as e:
            print(f"Error processing {url}: {e}")
            return None

    def process_krebs_articles(self, limit: int = 100):
        feed_entries = self.fetch_krebs_feed_entries()
        if not feed_entries:
            print("No feed entries found.")
            return

        processed_count = 0
        for entry in feed_entries:
            if processed_count >= limit:
                break

            print(f"Processing article: {entry['title']}")
            content = self.scrape_article(entry['link'])
            if not content:
                print(f"Failed to scrape content for {entry['link']}\n")
                continue

            if self.is_duplicate(entry['link'], entry['title'], content):
                print("Skipping duplicate article")
                continue

            try:
                with sqlite3.connect(self.db_name) as conn:
                    c = conn.cursor()
                    c.execute("""
                        INSERT OR REPLACE INTO articles 
                        (link, title, published_date, content, source)
                        VALUES (?, ?, ?, ?, ?)
                    """, (
                        entry['link'],
                        entry['title'],
                        entry['published_date'],
                        content,
                        "krebs"
                    ))
                    conn.commit()
                
                print(f"Stored article: {entry['title']}")
                print(f"\nLink: {entry['link']}")
                print(f"Published: {entry['published_date']}")
                print(f"**{entry['title']}**")
                print(content.strip())
                print("\n---\n")
                
                processed_count += 1
                time.sleep(2)
            except sqlite3.Error as db_error:
                print(f"Database error while storing article: {db_error}")
                continue

def main():
    scraper = KrebsScraper(
        db_name='db/news.db',
        feed_url='https://krebsonsecurity.com/feed/'
    )
    scraper.process_krebs_articles(limit=100)

if __name__ == "__main__":
    main()

================
File: scrapers/nist.py
================
import sqlite3
import requests
from bs4 import BeautifulSoup
import feedparser
import time
import sys
from typing import Optional, Dict, Any, List
from difflib import SequenceMatcher

class NISTCybersecurityNewsScraper:
    def __init__(self,
                 db_name: str = 'db/news.db',
                 feed_url: str = "https://www.nist.gov/news-events/cybersecurity/rss.xml"):
        self.db_name = db_name
        self.feed_url = feed_url
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': ('Mozilla/5.0 (Windows NT 10.0; Win64; x64) '
                           'AppleWebKit/537.36 (KHTML, like Gecko) '
                           'Chrome/91.0.4472.124 Safari/537.36')
        })
        self.setup_database()

    def setup_database(self):
        try:
            with sqlite3.connect(self.db_name) as conn:
                c = conn.cursor()
                c.execute("""
                    CREATE TABLE IF NOT EXISTS articles (
                        link TEXT PRIMARY KEY,
                        title TEXT NOT NULL,
                        published_date TIMESTAMP,
                        content TEXT NOT NULL,
                        source TEXT NOT NULL,
                        processed_date TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                    )
                """)
                conn.commit()
        except sqlite3.Error as e:
            sys.exit(f"Database initialization error: {e}")

    def clean_text(self, text: str) -> str:
        if not text:
            return ""
        return ' '.join(text.lower().split())

    def is_similar_content(self, t1: str, t2: str, threshold: float = 0.85) -> bool:
        return SequenceMatcher(None, self.clean_text(t1), self.clean_text(t2)).ratio() > threshold

    def is_duplicate(self, link: str, title: str, content: str) -> bool:
        try:
            with sqlite3.connect(self.db_name) as conn:
                c = conn.cursor()
                c.execute("SELECT link FROM articles WHERE link = ?", (link,))
                if c.fetchone():
                    print(f"Duplicate found (exact link match): {link}")
                    return True

                c.execute("SELECT title, content FROM articles WHERE source = 'nist'")
                existing = c.fetchall()
                ct = self.clean_text(title)
                cc = self.clean_text(content)
                for et, ec in existing:
                    if self.is_similar_content(ct, et, 0.9):
                        if self.is_similar_content(cc, ec, 0.85):
                            print(f"Duplicate found (similar content): {link}")
                            return True
                return False
        except sqlite3.Error as e:
            print(f"Database error while checking duplicates: {e}")
            return False

    def fetch_feed_entries(self) -> List[Dict[str, Any]]:
        try:
            feed = feedparser.parse(self.feed_url)
            entries = []
            for entry in feed.entries:
                if "/news-events/news/" in entry.link:
                    entries.append({
                        'link': entry.link,
                        'title': entry.title,
                        'published_date': getattr(entry, 'published', entry.get('pubdate', None))
                    })
            return entries
        except Exception as e:
            print(f"Error fetching feed entries: {e}")
            return []

    def scrape_article(self, url: str) -> Optional[str]:
        try:
            response = self.session.get(url, timeout=10)
            response.raise_for_status()
            soup = BeautifulSoup(response.content, 'html.parser')

            article_section = soup.find('section', class_='nist-page__content usa-section clearfix')
            if not article_section:
                return None

            paragraphs = article_section.find_all('p')
            article_text = '\n\n'.join(p.get_text().strip() for p in paragraphs if p.get_text().strip())
            return article_text if article_text else None

        except requests.RequestException as e:
            print(f"Request error while scraping {url}: {e}")
            return None
        except Exception as e:
            print(f"Error processing {url}: {e}")
            return None

    def process_articles(self, limit: int = 100):
        feed_entries = self.fetch_feed_entries()
        if not feed_entries:
            print("No feed entries found or no news events.")
            return

        processed = 0
        for entry in feed_entries:
            if processed >= limit:
                break
            print(f"\nProcessing article: {entry['title']}")
            content = self.scrape_article(entry['link'])
            if not content:
                print(f"Failed to scrape content for {entry['link']}\n")
                continue

            if self.is_duplicate(entry['link'], entry['title'], content):
                print("Skipping duplicate article")
                continue

            try:
                with sqlite3.connect(self.db_name) as conn:
                    c = conn.cursor()
                    c.execute("""
                        INSERT OR REPLACE INTO articles
                        (link, title, published_date, content, source)
                        VALUES (?, ?, ?, ?, ?)
                    """, (
                        entry['link'],
                        entry['title'],
                        entry['published_date'],
                        content,
                        "nist"
                    ))
                    conn.commit()

                print(f"Stored article: {entry['title']}")
                print(f"Link: {entry['link']}")
                print(f"Published: {entry['published_date']}")
                print("\nContent Preview:")
                print(content[:500] + "...\n")
                print("-" * 80 + "\n")

                processed += 1
                time.sleep(2)
            except sqlite3.Error as db_error:
                print(f"Database error while storing article: {db_error}")
                continue

def main():
    scraper = NISTCybersecurityNewsScraper()
    scraper.process_articles(limit=100)

if __name__ == "__main__":
    main()

================
File: scrapers/register-scraper.py
================
import sqlite3
import requests
from bs4 import BeautifulSoup
import feedparser
import time
import sys
from typing import Optional, Dict, Any, List
from difflib import SequenceMatcher

class RegisterScraper:
    def __init__(self, 
                 db_name: str = 'db/news.db', 
                 feed_url: str = "https://www.theregister.com/headlines.atom"):
        self.db_name = db_name
        self.feed_url = feed_url
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': ('Mozilla/5.0 (Windows NT 10.0; Win64; x64) '
                           'AppleWebKit/537.36 (KHTML, like Gecko) '
                           'Chrome/91.0.4472.124 Safari/537.36')
        })
        self.setup_database()

    def setup_database(self):
        try:
            with sqlite3.connect(self.db_name) as conn:
                c = conn.cursor()
                c.execute("""
                    CREATE TABLE IF NOT EXISTS articles (
                        link TEXT PRIMARY KEY,
                        title TEXT NOT NULL,
                        published_date TIMESTAMP,
                        content TEXT NOT NULL,
                        source TEXT NOT NULL,
                        processed_date TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                    )
                """)
                conn.commit()
        except sqlite3.Error as e:
            sys.exit(f"Database initialization error: {e}")

    def clean_text(self, text: str) -> str:
        if not text:
            return ""
        return ' '.join(text.lower().split())

    def is_similar_content(self, t1: str, t2: str, threshold: float = 0.85) -> bool:
        return SequenceMatcher(None, self.clean_text(t1), self.clean_text(t2)).ratio() > threshold

    def is_duplicate(self, link: str, title: str, content: str) -> bool:
        try:
            with sqlite3.connect(self.db_name) as conn:
                c = conn.cursor()
                c.execute("SELECT link FROM articles WHERE link = ?", (link,))
                if c.fetchone():
                    print(f"Duplicate found (exact link match): {link}")
                    return True

                c.execute("SELECT title, content FROM articles WHERE source = 'register'")
                existing = c.fetchall()

                ct = self.clean_text(title)
                cc = self.clean_text(content)
                for et, ec in existing:
                    if self.is_similar_content(ct, et, 0.9):
                        if self.is_similar_content(cc, ec, 0.85):
                            print(f"Duplicate found (similar content): {link}")
                            return True
                return False
        except sqlite3.Error as e:
            print(f"Database error while checking duplicates: {e}")
            return False

    def fetch_register_feed_entries(self) -> List[Dict[str, Any]]:
        try:
            feed = feedparser.parse(self.feed_url)
            entries = []
            for entry in feed.entries:
                link = getattr(entry, 'link', None)
                title = getattr(entry, 'title', "No Title")
                published = getattr(entry, 'published', getattr(entry, 'updated', None))
                if link:
                    entries.append({
                        'link': link,
                        'title': title,
                        'published_date': published
                    })
            return entries
        except Exception as e:
            print(f"Error fetching feed entries: {e}")
            return []

    def scrape_article(self, url: str) -> Optional[str]:
        try:
            response = self.session.get(url, timeout=10)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            article_div = soup.find('div', id='article')
            if not article_div:
                return None
            
            body_div = article_div.find('div', id='body')
            if not body_div:
                return None

            for div in body_div.find_all('div', class_=['adun', 'wptl', 'listinks']):
                div.decompose()

            paragraphs = body_div.find_all('p')
            article_text = '\n'.join(p.get_text().strip() for p in paragraphs if p.get_text().strip())
            return article_text if article_text else None

        except requests.RequestException as e:
            print(f"Request error while scraping {url}: {e}")
            return None
        except Exception as e:
            print(f"Error processing {url}: {e}")
            return None

    def process_register_articles(self, limit: int = 10):
        feed_entries = self.fetch_register_feed_entries()
        if not feed_entries:
            print("No feed entries found.")
            return

        processed_count = 0
        for entry in feed_entries:
            if processed_count >= limit:
                break

            print(f"\nProcessing article: {entry['title']}")
            content = self.scrape_article(entry['link'])
            if not content:
                print(f"Failed to scrape content for {entry['link']}")
                continue

            if self.is_duplicate(entry['link'], entry['title'], content):
                print("Skipping duplicate article")
                continue

            try:
                with sqlite3.connect(self.db_name) as conn:
                    c = conn.cursor()
                    c.execute("""
                        INSERT OR REPLACE INTO articles 
                        (link, title, published_date, content, source)
                        VALUES (?, ?, ?, ?, ?)
                    """, (
                        entry['link'],
                        entry['title'],
                        entry['published_date'],
                        content,
                        "register"
                    ))
                    conn.commit()
                
                print(f"Stored article: {entry['title']}")
                print(f"Link: {entry['link']}")
                print(f"Published: {entry['published_date']}")
                print("Content Preview:")
                print(content[:500] + "...\n")
                print("-" * 80)
                
                processed_count += 1
                time.sleep(2)
            except sqlite3.Error as db_error:
                print(f"Database error while storing article: {db_error}")
                continue

def main():
    scraper = RegisterScraper()
    scraper.process_register_articles(limit=100)

if __name__ == "__main__":
    main()

================
File: scrapers/schneier-scraper.py
================
import sqlite3
import requests
from bs4 import BeautifulSoup
from typing import Optional, Dict, Any, List
import time
import xml.etree.ElementTree as ET
from difflib import SequenceMatcher

class CybersecurityScraper:
    def __init__(self,
                 db_name: str = 'db/news.db',
                 site_config: Dict[str, Any] = None):
        self.db_name = db_name
        self.site_config = site_config or {}
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': ('Mozilla/5.0 (Windows NT 10.0; Win64; x64) '
                           'AppleWebKit/537.36 (KHTML, like Gecko) '
                           'Chrome/91.0.4472.124 Safari/537.36')
        })
        self.setup_database()

    def setup_database(self):
        with sqlite3.connect(self.db_name) as conn:
            c = conn.cursor()
            c.execute("""
                CREATE TABLE IF NOT EXISTS articles (
                    link TEXT PRIMARY KEY,
                    title TEXT NOT NULL,
                    published_date TIMESTAMP,
                    content TEXT,
                    source TEXT NOT NULL,
                    processed_date TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                )
            """)
            conn.commit()

    def clean_text(self, text: str) -> str:
        if not text:
            return ""
        return ' '.join(text.lower().split())

    def is_similar_content(self, text1: str, text2: str, threshold: float = 0.85) -> bool:
        return SequenceMatcher(None, self.clean_text(text1), self.clean_text(text2)).ratio() > threshold

    def is_duplicate(self, link: str, title: str, content: str, source_name: str) -> bool:
        try:
            with sqlite3.connect(self.db_name) as conn:
                c = conn.cursor()
                c.execute("SELECT link FROM articles WHERE link = ?", (link,))
                if c.fetchone():
                    return True

                c.execute("SELECT title, content FROM articles WHERE source = ?", (source_name,))
                existing_articles = c.fetchall()

                ct = self.clean_text(title)
                cc = self.clean_text(content)

                for et, ec in existing_articles:
                    if self.is_similar_content(ct, et, 0.9):
                        if self.is_similar_content(cc, ec, 0.85):
                            return True
                return False
        except sqlite3.Error:
            return False

    def parse_atom_feed(self, feed_content: str) -> List[Dict[str, Any]]:
        try:
            root = ET.fromstring(feed_content)
            ns = {'atom': 'http://www.w3.org/2005/Atom'}
            entries = []

            for entry in root.findall('atom:entry', ns):
                link_elem = entry.find('atom:link[@rel="alternate"]', ns)
                link = link_elem.get('href') if link_elem is not None else None

                title_elem = entry.find('atom:title', ns)
                title = title_elem.text if title_elem is not None else None

                published_elem = entry.find('atom:published', ns)
                published = published_elem.text if published_elem is not None else None

                content = ''
                content_elem = entry.find('atom:content', ns)
                if content_elem is not None:
                    if content_elem.get('type') == 'html':
                        soup = BeautifulSoup(content_elem.text or '', 'html.parser')
                        content_parts = []
                        for tag in soup.find_all(['p', 'blockquote'], recursive=False):
                            if not any(cls in (tag.get('class') or []) for cls in ['entry-tags', 'posted']):
                                content_parts.append(tag.get_text().strip())
                        content = '\n'.join(filter(None, content_parts))
                    else:
                        content = content_elem.text or ''

                if link:
                    entries.append({
                        'link': link,
                        'title': title,
                        'published_date': published,
                        'content': content
                    })
            return entries
        except (ET.ParseError, Exception):
            return []

    def process_feed(self, feed_url: str, source_name: str):
        try:
            response = self.session.get(feed_url, timeout=10)
            response.raise_for_status()
            articles = self.parse_atom_feed(response.text)

            seen_links = set()
            for article in articles:
                if article['link'] in seen_links:
                    continue
                seen_links.add(article['link'])

                if not all(k in article for k in ['link', 'title', 'published_date']):
                    continue

                if self.is_duplicate(article['link'], article['title'], article['content'], source_name):
                    continue

                with sqlite3.connect(self.db_name) as conn:
                    c = conn.cursor()
                    c.execute("""
                        INSERT OR REPLACE INTO articles
                        (link, title, published_date, content, source)
                        VALUES (?, ?, ?, ?, ?)
                    """, (
                        article['link'],
                        article['title'],
                        article['published_date'],
                        article['content'],
                        source_name
                    ))
                    conn.commit()

                print(f"Link: {article['link']}")
                print(f"Published: {article['published_date']}")
                print(f"**{article['title']}**")
                print(article['content'].strip())
                print("\n---\n")

                time.sleep(1)
        except requests.RequestException:
            pass

def main():
    site_configs = {
        "schneier": {
            "feed_url": "https://www.schneier.com/feed/atom/"
        }
    }
    scraper = CybersecurityScraper(site_config=site_configs)
    scraper.process_feed(site_configs["schneier"]["feed_url"], "schneier")

if __name__ == "__main__":
    main()

================
File: scrapers/Scrapinghackernews.py
================
#!/usr/bin/env python3
import argparse
import sqlite3
import requests
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry
from bs4 import BeautifulSoup
import logging
import sys
import time
import xml.etree.ElementTree as ET
import re
from email.utils import parsedate_to_datetime
from typing import Optional, Dict, Any, List
from difflib import SequenceMatcher

class THNScraper:
    def __init__(self, db_name: str, feed_url: str, batch_size: int, rate_limit: float, log_level: str):
        self.db_name = db_name
        self.feed_url = feed_url
        self.batch_size = batch_size
        self.rate_limit = rate_limit
        self.logger = self.setup_logging(log_level)
        self.setup_database()
        self.session = self.setup_http_session()

    def setup_logging(self, log_level: str):
        logger = logging.getLogger("THNScraper")
        logger.setLevel(getattr(logging, log_level.upper(), logging.INFO))

        formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')
        file_handler = logging.FileHandler('thn_scraper_no_desc.log')
        file_handler.setFormatter(formatter)
        console_handler = logging.StreamHandler(sys.stdout)
        console_handler.setFormatter(formatter)

        if not logger.handlers:
            logger.addHandler(file_handler)
            logger.addHandler(console_handler)

        return logger

    def setup_database(self):
        try:
            self.conn = sqlite3.connect(self.db_name)
            self.conn.row_factory = sqlite3.Row
            self.cursor = self.conn.cursor()
            self.cursor.execute("""
                CREATE TABLE IF NOT EXISTS articles (
                    link TEXT PRIMARY KEY,
                    title TEXT NOT NULL,
                    published_date TEXT,
                    content TEXT,
                    source TEXT NOT NULL,
                    processed_date TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                )
            """)
            self.conn.commit()
            self.logger.info("Database initialized successfully.")
        except sqlite3.Error:
            self.logger.exception("Database initialization error")
            sys.exit(1)

    def setup_http_session(self) -> requests.Session:
        session = requests.Session()
        retries = Retry(
            total=3,
            backoff_factor=1,
            status_forcelist=[500, 502, 503, 504],
            allowed_methods=["HEAD", "GET", "OPTIONS"]
        )
        adapter = HTTPAdapter(max_retries=retries)
        session.mount("http://", adapter)
        session.mount("https://", adapter)
        session.headers.update({
            'User-Agent': ('Mozilla/5.0 (Windows NT 10.0; Win64; x64) '
                           'AppleWebKit/537.36 (KHTML, like Gecko) '
                           'Chrome/91.0.4472.124 Safari/537.36')
        })
        return session

    def parse_rss_feed(self) -> List[Dict[str, Any]]:
        try:
            response = self.session.get(self.feed_url, timeout=10)
            response.raise_for_status()
            root = ET.fromstring(response.content)
            entries = []
            for item in root.findall('.//item'):
                pub_date = None
                pub_date_elem = item.find('pubDate')
                if pub_date_elem is not None and pub_date_elem.text:
                    try:
                        dt = parsedate_to_datetime(pub_date_elem.text)
                        pub_date = dt.isoformat()
                    except Exception:
                        self.logger.warning(f"Error parsing date {pub_date_elem.text}", exc_info=True)
                entry = {
                    'title': item.find('title').text if item.find('title') is not None else None,
                    'link': item.find('link').text if item.find('link') is not None else None,
                    'published_date': pub_date,
                }
                entries.append(entry)
            return entries
        except requests.RequestException:
            self.logger.exception("Error fetching RSS feed")
            return []
        except ET.ParseError:
            self.logger.exception("Error parsing RSS XML")
            return []

    def scrape_article(self, url: str) -> Optional[Dict[str, Any]]:
        try:
            response = self.session.get(url, timeout=10)
            response.raise_for_status()
            soup = BeautifulSoup(response.content, 'html.parser')
            article_div = soup.find('div', {'class': 'articlebody', 'id': 'articlebody'})
            if not article_div:
                self.logger.warning(f"Article content not found for URL: {url}")
                return None

            elements_to_remove = [
                ('div', {'class': ['dog_two', 'note-b', 'stophere']}),
                ('div', {'id': ['hiddenH1']}),
                ('center', {}),
                ('div', {'class': 'separator'}),
            ]
            for tag, attrs in elements_to_remove:
                for element in article_div.find_all(tag, attrs=attrs):
                    element.decompose()

            paragraphs = [
                p.get_text().strip() for p in article_div.find_all('p') if p.get_text().strip()
            ]
            return {'content': "\n\n".join(paragraphs)}
        except requests.RequestException:
            self.logger.exception(f"Error fetching article URL: {url}")
            return None
        except Exception:
            self.logger.exception(f"Error processing article URL: {url}")
            return None

    def remove_emojis(self, text: Optional[str]) -> str:
        if not text:
            return ""
        emoji_pattern = re.compile(
            "["
            "\U0001F600-\U0001F64F"
            "\U0001F300-\U0001F5FF"
            "\U0001F680-\U0001F6FF"
            "\U0001F1E0-\U0001F1FF"
            "\U00002702-\U000027B0"
            "\U000024C2-\U0001F251"
            "]+",
            flags=re.UNICODE,
        )
        return emoji_pattern.sub(r"", text)

    def clean_text(self, text: str) -> str:
        if not text:
            return ""
        return ' '.join(text.lower().split())

    def is_similar_content(self, t1: str, t2: str, threshold: float = 0.85) -> bool:
        return SequenceMatcher(None, self.clean_text(t1), self.clean_text(t2)).ratio() > threshold

    def is_duplicate(self, link: str, title: str, content: str) -> bool:
        try:
            self.cursor.execute("SELECT link FROM articles WHERE link = ?", (link,))
            if self.cursor.fetchone():
                self.logger.info(f"Duplicate found (exact link match): {link}")
                return True

            self.cursor.execute("SELECT title, content FROM articles WHERE source = 'TheHackerNews'")
            existing_articles = self.cursor.fetchall()

            ct = self.clean_text(title)
            cc = self.clean_text(content)
            for row in existing_articles:
                existing_title = row['title']
                existing_content = row['content']
                if self.is_similar_content(ct, existing_title, 0.9):
                    if self.is_similar_content(cc, existing_content, 0.85):
                        self.logger.info(f"Duplicate found (similar content): {link}")
                        return True
            return False
        except sqlite3.Error as e:
            self.logger.error(f"Database error while checking duplicates: {e}")
            return False

    def insert_article(self, link: str, title: str, published_date: Optional[str], content: str, source: str = "TheHackerNews"):
        try:
            self.cursor.execute("""
                INSERT OR REPLACE INTO articles (link, title, published_date, content, source)
                VALUES (?, ?, ?, ?, ?)
            """, (link, title, published_date, content, source))
            self.conn.commit()
            self.logger.info(f"Successfully inserted article: {title}")
        except sqlite3.Error:
            self.logger.exception(f"Database insertion error for article: {title}")

    def process_all_articles(self):
        entries = self.parse_rss_feed()
        if not entries:
            self.logger.info("No entries found in feed")
            return

        self.logger.info(f"Found {len(entries)} entries in feed")
        for i in range(0, len(entries), self.batch_size):
            batch = entries[i:i + self.batch_size]
            for entry in batch:
                if not entry.get('link'):
                    continue
                title = self.remove_emojis(entry.get('title'))
                self.logger.info(f"Processing article: {title}")

                article_data = self.scrape_article(entry['link'])
                if article_data:
                    content = self.remove_emojis(article_data.get('content', ''))

                    if self.is_duplicate(entry['link'], title, content):
                        self.logger.info("Skipping duplicate article")
                        continue

                    pub_date = entry.get('published_date')
                    self.insert_article(entry['link'], title, pub_date, content)
                else:
                    self.logger.warning(f"Failed to scrape article: {entry['link']}")

                time.sleep(self.rate_limit)

    def close(self):
        try:
            self.conn.close()
        except Exception:
            self.logger.exception("Error closing database connection")
        self.session.close()

def main():
    parser = argparse.ArgumentParser(description="The Hacker News Scraper")
    parser.add_argument("--db", type=str, default="db/news.db", help="SQLite database file name")
    parser.add_argument("--feed_url", type=str, default="https://feeds.feedburner.com/TheHackersNews")
    parser.add_argument("--batch_size", type=int, default=5)
    parser.add_argument("--rate_limit", type=float, default=2.0)
    parser.add_argument("--log_level", type=str, default="INFO")
    args = parser.parse_args()

    scraper = THNScraper(
        db_name=args.db,
        feed_url=args.feed_url,
        batch_size=args.batch_size,
        rate_limit=args.rate_limit,
        log_level=args.log_level
    )
    try:
        scraper.process_all_articles()
    except KeyboardInterrupt:
        scraper.logger.info("Processing interrupted by user.")
    except Exception:
        scraper.logger.exception("An unexpected error occurred.")
    finally:
        scraper.close()

if __name__ == "__main__":
    main()

================
File: scrapers/securelist-scraper.py
================
import sqlite3
import requests
import feedparser
from bs4 import BeautifulSoup
import logging
from typing import Optional, Dict, Any, List
import time
import sys
import re
from difflib import SequenceMatcher

class SecurelistProcessor:
    def __init__(self, db_name: str = 'db/news.db'):
        self.db_name = db_name
        self.feed_url = "https://securelist.com/feed/"
        self.logger = self.setup_logging()
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        })
        self.setup_database()

    def setup_logging(self) -> logging.Logger:
        logger = logging.getLogger(__name__)
        logger.setLevel(logging.INFO)

        fmt = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')
        fh = logging.FileHandler('securelist_scraper.log')
        fh.setFormatter(fmt)
        ch = logging.StreamHandler(sys.stdout)
        ch.setFormatter(fmt)

        if not logger.handlers:
            logger.addHandler(fh)
            logger.addHandler(ch)
        return logger

    def setup_database(self):
        try:
            with sqlite3.connect(self.db_name) as conn:
                c = conn.cursor()
                c.execute("""
                    CREATE TABLE IF NOT EXISTS articles (
                        link TEXT PRIMARY KEY,
                        title TEXT NOT NULL,
                        published_date TIMESTAMP,
                        content TEXT NOT NULL,
                        source TEXT NOT NULL,
                        processed_date TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                    )
                """)
                conn.commit()
        except sqlite3.Error as e:
            self.logger.error(f"Database initialization error: {e}")
            raise

    def clean_text(self, text: str) -> str:
        if not text:
            return ""
        return ' '.join(text.lower().split())

    def is_similar_content(self, t1: str, t2: str, threshold: float = 0.85) -> bool:
        return SequenceMatcher(None, self.clean_text(t1), self.clean_text(t2)).ratio() > threshold

    def is_duplicate(self, link: str, title: str, content: str) -> bool:
        try:
            with sqlite3.connect(self.db_name) as conn:
                c = conn.cursor()
                c.execute("SELECT link FROM articles WHERE link = ?", (link,))
                if c.fetchone():
                    self.logger.info(f"Duplicate found (exact link match): {link}")
                    return True

                c.execute("SELECT title, content FROM articles WHERE source = 'securelist'")
                existing = c.fetchall()

                ct = self.clean_text(title)
                cc = self.clean_text(content)
                for et, ec in existing:
                    if self.is_similar_content(ct, et, 0.9):
                        if self.is_similar_content(cc, ec, 0.85):
                            self.logger.info(f"Duplicate found (similar content): {link}")
                            return True
                return False
        except sqlite3.Error as e:
            self.logger.error(f"Database error while checking duplicates: {e}")
            return False

    def fetch_feed(self) -> List[Dict]:
        try:
            self.logger.info(f"Fetching RSS feed from {self.feed_url}")
            response = self.session.get(self.feed_url, timeout=10)
            response.raise_for_status()
            feed = feedparser.parse(response.text)

            if feed.bozo != 0:
                self.logger.error(f"Feed parsing error: {feed.bozo_exception}")
                return []

            articles = []
            for entry in feed.entries:
                article = {
                    'title': entry.get('title', ''),
                    'link': entry.get('link', ''),
                    'date': entry.get('published', '')
                }
                articles.append(article)
            self.logger.info(f"Found {len(articles)} articles in feed")
            return articles
        except requests.RequestException as e:
            self.logger.error(f"Error fetching feed: {e}")
            return []
        except Exception as e:
            self.logger.error(f"Unexpected error: {e}")
            return []

    def scrape_article(self, url: str) -> Optional[str]:
        try:
            response = self.session.get(url, timeout=10)
            response.raise_for_status()

            soup = BeautifulSoup(response.content, 'html.parser')
            article_div = soup.find('div', class_='js-reading-content')
            if not article_div:
                self.logger.warning(f"Could not find article content for {url}")
                return None

            content_div = article_div.find('div', class_='c-wysiwyg')
            if not content_div:
                self.logger.warning(f"Could not find content div for {url}")
                return None

            for element in content_div.find_all('div', class_=['wp-caption', 'js-infogram-embed']):
                element.decompose()

            content_elements = content_div.find_all(['p', 'h1', 'h2', 'h3', 'h4', 'h5', 'h6'])
            article_text = ""
            for element in content_elements:
                text = element.get_text().strip()
                if text:
                    if element.name.startswith('h'):
                        article_text += f"\n\n{text}\n"
                    else:
                        article_text += f"{text}\n"

            return article_text.strip()
        except requests.RequestException as e:
            self.logger.error(f"Error fetching {url}: {e}")
            return None
        except Exception as e:
            self.logger.error(f"Error processing {url}: {e}")
            return None

    def process_article(self, article: Dict):
        link = article['link']
        title = article['title']
        published_date = article['date']

        self.logger.info(f"Processing: {title}")
        content = self.scrape_article(link)
        if not content:
            return

        if self.is_duplicate(link, title, content):
            self.logger.info(f"Skipping duplicate article: {title}")
            return

        try:
            with sqlite3.connect(self.db_name) as conn:
                c = conn.cursor()
                c.execute("""
                    INSERT OR REPLACE INTO articles
                    (link, title, published_date, content, source)
                    VALUES (?, ?, ?, ?, ?)
                """, (
                    link,
                    title,
                    published_date,
                    content,
                    "securelist"
                ))
                conn.commit()
            self.logger.info(f"Stored article: {title}")
            print(f"Link: {link}")
            print(f"Published: {published_date}")
            print(f"**{title}**")
            print(content.strip())
            print("\n---\n")
        except sqlite3.Error as e:
            self.logger.error(f"Database error storing article {title}: {e}")

    def process_all_articles(self, limit: int = 100):
        try:
            articles = self.fetch_feed()
            if not articles:
                self.logger.info("No articles fetched from feed.")
                return

            processed_count = 0
            for article in articles:
                if processed_count >= limit:
                    self.logger.info(f"Reached processing limit of {limit} articles.")
                    break

                self.process_article(article)
                processed_count += 1
                time.sleep(2)

            with sqlite3.connect(self.db_name) as conn:
                c = conn.cursor()
                c.execute("SELECT COUNT(*) FROM articles WHERE source = 'securelist'")
                total_count = c.fetchone()[0]
                self.logger.info("\nFinal Statistics:")
                self.logger.info(f"Total articles in database from Securelist: {total_count}")

        except Exception as e:
            self.logger.error(f"Error processing articles: {e}")

def main():
    processor = SecurelistProcessor()
    processor.process_all_articles(limit=100)

if __name__ == "__main__":
    main()

================
File: scrapers/Slashdotit.py
================
import sqlite3
import requests
from bs4 import BeautifulSoup
import feedparser
import time
import sys
from datetime import datetime
from typing import Optional, Dict, Any, List

class SlashdotITNewsScraper:
    def __init__(self,
                 db_name: str = 'db/news.db',
                 feed_url: str = "https://rss.slashdot.org/Slashdot/slashdotit"):
        self.db_name = db_name
        self.feed_url = feed_url
        self.session = requests.Session()
        self.session.headers.update({
            "User-Agent": ("Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
                           "AppleWebKit/537.36 (KHTML, like Gecko) "
                           "Chrome/91.0.4472.124 Safari/537.36")
        })
        self.setup_database()

    def setup_database(self):
        try:
            with sqlite3.connect(self.db_name) as conn:
                c = conn.cursor()
                c.execute("""
                    CREATE TABLE IF NOT EXISTS articles (
                        link TEXT PRIMARY KEY,
                        title TEXT NOT NULL,
                        published_date TIMESTAMP,
                        content TEXT NOT NULL,
                        source TEXT NOT NULL,
                        processed_date TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                    )
                """)
                conn.commit()
        except sqlite3.Error as e:
            sys.exit(f"Database initialization error: {e}")

    def fetch_feed_entries(self) -> List[Dict[str, Any]]:
        try:
            feed = feedparser.parse(self.feed_url)
            entries = []
            for entry in feed.entries:
                published = entry.get('published', entry.get('dc_date', None))
                entries.append({
                    'link': entry.link,
                    'title': entry.title,
                    'published_date': published
                })
            return entries
        except Exception as e:
            print(f"Error fetching feed entries: {e}")
            return []

    def scrape_article(self, url: str) -> Optional[str]:
        try:
            response = self.session.get(url, timeout=10)
            response.raise_for_status()
            soup = BeautifulSoup(response.content, "html.parser")

            content_div = None
            body_div = soup.find("div", class_="body")
            if body_div:
                content_div = body_div.find("div", class_="p")
            if not content_div:
                content_div = soup.find("div", class_="p")
            if not content_div:
                print(f"Could not locate article content at {url}")
                return None

            paragraphs = content_div.find_all("p")
            article_text = "\n\n".join(
                p.get_text().strip() for p in paragraphs if p.get_text().strip()
            )
            if not article_text:
                article_text = content_div.get_text().strip()
            return article_text if article_text else None

        except requests.RequestException as e:
            print(f"Request error while scraping {url}: {e}")
            return None
        except Exception as e:
            print(f"Error processing {url}: {e}")
            return None

    def already_processed(self, link: str) -> bool:
        try:
            with sqlite3.connect(self.db_name) as conn:
                c = conn.cursor()
                c.execute("SELECT link FROM articles WHERE link = ?", (link,))
                row = c.fetchone()
                return bool(row)
        except sqlite3.Error as e:
            print(f"Database error: {e}")
            return False

    def process_articles(self, limit: int = 10):
        feed_entries = self.fetch_feed_entries()
        if not feed_entries:
            print("No feed entries found.")
            return

        new_entries = []
        for entry in feed_entries:
            if not self.already_processed(entry['link']):
                new_entries.append(entry)
            if len(new_entries) >= limit:
                break

        if not new_entries:
            print("No new articles to process.")
            return

        for entry in new_entries:
            print(f"\nProcessing article: {entry['title']}")
            content = self.scrape_article(entry['link'])
            if not content:
                print(f"Failed to retrieve content for {entry['link']}\n")
                continue

            pub_date = entry['published_date'] or datetime.now().strftime("%Y-%m-%d %H:%M:%S")
            try:
                with sqlite3.connect(self.db_name) as conn:
                    c = conn.cursor()
                    c.execute("""
                        INSERT OR REPLACE INTO articles
                        (link, title, published_date, content, source)
                        VALUES (?, ?, ?, ?, ?)
                    """, (
                        entry['link'],
                        entry['title'],
                        pub_date,
                        content,
                        "slashdot_it"
                    ))
                    conn.commit()

                print("Title:", entry['title'])
                print("Link:", entry['link'])
                print("Published:", pub_date)
                print("\nContent Preview:")
                print(content[:500] + "...\n")
                print("-" * 80 + "\n")

                time.sleep(2)
            except sqlite3.Error as db_error:
                print(f"Database error while storing article: {db_error}")
                continue

def main():
    scraper = SlashdotITNewsScraper()
    scraper.process_articles(limit=10)

if __name__ == "__main__":
    main()

================
File: scrapers/sophos.py
================
import sqlite3
import requests
from bs4 import BeautifulSoup
import feedparser
import time
import sys
from typing import Optional, Dict, Any, List

class SophosNewsScraper:
    def __init__(self,
                 db_name: str = 'db/news.db',
                 feed_url: str = "https://news.sophos.com/en-us/feed/"):
        self.db_name = db_name
        self.feed_url = feed_url
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': ('Mozilla/5.0 (Windows NT 10.0; Win64; x64) '
                           'AppleWebKit/537.36 (KHTML, like Gecko) '
                           'Chrome/91.0.4472.124 Safari/537.36')
        })
        self.setup_database()

    def setup_database(self):
        try:
            with sqlite3.connect(self.db_name) as conn:
                c = conn.cursor()
                c.execute("""
                    CREATE TABLE IF NOT EXISTS articles (
                        link TEXT PRIMARY KEY,
                        title TEXT NOT NULL,
                        published_date TIMESTAMP,
                        content TEXT NOT NULL,
                        source TEXT NOT NULL,
                        processed_date TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                    )
                """)
                conn.commit()
        except sqlite3.Error as e:
            sys.exit(f"Database initialization error: {e}")

    def fetch_feed_entries(self) -> List[Dict[str, Any]]:
        try:
            feed = feedparser.parse(self.feed_url)
            entries = []
            for entry in feed.entries:
                entries.append({
                    'link': entry.link,
                    'title': entry.title,
                    'published_date': getattr(entry, 'published', entry.get('pubdate'))
                })
            return entries
        except Exception as e:
            print(f"Error fetching feed entries: {e}")
            return []

    def scrape_article(self, url: str) -> Optional[str]:
        try:
            response = self.session.get(url, timeout=10)
            response.raise_for_status()
            soup = BeautifulSoup(response.content, 'html.parser')

            article_body = soup.find('div', class_='entry-content lg:prose-lg mx-auto prose max-w-4xl')
            if not article_body:
                return None

            paragraphs = article_body.find_all('p')
            article_text = '\n\n'.join(p.get_text().strip() for p in paragraphs if p.get_text().strip())
            return article_text if article_text else None
        except requests.RequestException as e:
            print(f"Request error while scraping {url}: {e}")
            return None
        except Exception as e:
            print(f"Error processing {url}: {e}")
            return None

    def already_processed(self, link: str) -> bool:
        try:
            with sqlite3.connect(self.db_name) as conn:
                c = conn.cursor()
                c.execute("SELECT link FROM articles WHERE link = ?", (link,))
                row = c.fetchone()
                return bool(row)
        except sqlite3.Error:
            return False

    def process_articles(self, limit: int = 100):
        feed_entries = self.fetch_feed_entries()
        if not feed_entries:
            print("No feed entries found.")
            return

        new_entries = []
        for entry in feed_entries:
            if not self.already_processed(entry['link']):
                new_entries.append(entry)
            if len(new_entries) >= limit:
                break

        if not new_entries:
            print("No new articles to process.")
            return

        for entry in new_entries:
            print(f"\nProcessing article: {entry['title']}")
            content = self.scrape_article(entry['link'])
            if not content:
                print(f"Failed to scrape content for {entry['link']}\n")
                continue

            try:
                with sqlite3.connect(self.db_name) as conn:
                    c = conn.cursor()
                    c.execute("""
                        INSERT OR REPLACE INTO articles
                        (link, title, published_date, content, source)
                        VALUES (?, ?, ?, ?, ?)
                    """, (
                        entry['link'],
                        entry['title'],
                        entry['published_date'],
                        content,
                        "sophos"
                    ))
                    conn.commit()

                print(f"Title: {entry['title']}")
                print(f"Link: {entry['link']}")
                print(f"Published: {entry['published_date']}")
                print("\nContent Preview:")
                print(content[:500] + "...\n")
                print("-" * 80 + "\n")

            except sqlite3.Error as db_error:
                print(f"Database error while storing article: {db_error}")
                continue

            time.sleep(2)

def main():
    scraper = SophosNewsScraper()
    scraper.process_articles(limit=100)

if __name__ == "__main__":
    main()

================
File: scrapers/techcrunch.py
================
import sqlite3
import requests
from bs4 import BeautifulSoup
import feedparser
import time
import sys
from typing import Optional, Dict, Any, List

class TechCrunchNewsScraper:
    def __init__(self,
                 db_name: str = 'db/news.db',
                 feed_url: str = "https://techcrunch.com/feed/"):
        self.db_name = db_name
        self.feed_url = feed_url
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': ('Mozilla/5.0 (Windows NT 10.0; Win64; x64) '
                           'AppleWebKit/537.36 (KHTML, like Gecko) '
                           'Chrome/91.0.4472.124 Safari/537.36')
        })
        self.setup_database()

    def setup_database(self):
        try:
            with sqlite3.connect(self.db_name) as conn:
                c = conn.cursor()
                c.execute("""
                CREATE TABLE IF NOT EXISTS articles (
                    link TEXT PRIMARY KEY,
                    title TEXT NOT NULL,
                    published_date TIMESTAMP,
                    content TEXT NOT NULL,
                    source TEXT NOT NULL,
                    processed_date TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                )
                """)
                conn.commit()
        except sqlite3.Error as e:
            sys.exit(f"Database initialization error: {e}")

    def fetch_feed_entries(self) -> List[Dict[str, Any]]:
        try:
            feed = feedparser.parse(self.feed_url)
            entries = []
            for entry in feed.entries:
                entries.append({
                    'link': entry.link,
                    'title': entry.title,
                    'published_date': getattr(entry, 'published', None)
                })
            return entries
        except Exception as e:
            print(f"Error fetching feed entries: {e}")
            return []

    def scrape_article(self, url: str) -> Optional[str]:
        try:
            response = self.session.get(url, timeout=10)
            response.raise_for_status()
            soup = BeautifulSoup(response.content, 'html.parser')

            content_div = soup.select_one("div.entry-content.wp-block-post-content")
            if not content_div:
                print(f"Unable to locate article content in: {url}")
                return None

            paragraphs = content_div.find_all("p")
            article_text = "\n\n".join(p.get_text().strip() for p in paragraphs if p.get_text().strip())
            return article_text if article_text else None
        except requests.RequestException as e:
            print(f"Request error while scraping {url}: {e}")
            return None
        except Exception as e:
            print(f"Error processing {url}: {e}")
            return None

    def already_processed(self, link: str) -> bool:
        try:
            with sqlite3.connect(self.db_name) as conn:
                c = conn.cursor()
                c.execute("SELECT link FROM articles WHERE link = ?", (link,))
                row = c.fetchone()
                return bool(row)
        except sqlite3.Error as e:
            print(f"Database error: {e}")
            return False

    def process_articles(self, limit: int = 10):
        feed_entries = self.fetch_feed_entries()
        if not feed_entries:
            print("No feed entries found.")
            return

        new_entries = []
        for entry in feed_entries:
            if not self.already_processed(entry['link']):
                new_entries.append(entry)
            if len(new_entries) >= limit:
                break

        if not new_entries:
            print("No new articles to process.")
            return

        for entry in new_entries:
            print(f"\nProcessing article: {entry['title']}")
            content = self.scrape_article(entry['link'])
            if not content:
                print(f"Failed to retrieve content for {entry['link']}\n")
                continue

            try:
                with sqlite3.connect(self.db_name) as conn:
                    c = conn.cursor()
                    c.execute("""
                    INSERT OR REPLACE INTO articles
                    (link, title, published_date, content, source)
                    VALUES (?, ?, ?, ?, ?)
                    """, (
                        entry['link'],
                        entry['title'],
                        entry['published_date'],
                        content,
                        "techcrunch"
                    ))
                    conn.commit()

                print("Title:", entry['title'])
                print("Link:", entry['link'])
                print("Published:", entry['published_date'])
                print("\nContent Preview:")
                print(content[:500] + "...\n")
                print("-" * 80 + "\n")

            except sqlite3.Error as db_error:
                print(f"Database error while storing the article: {db_error}")
                continue

            time.sleep(2)

def main():
    scraper = TechCrunchNewsScraper()
    scraper.process_articles(limit=10)

if __name__ == "__main__":
    main()

================
File: scrapers/techradar.py
================
import sqlite3
import requests
from bs4 import BeautifulSoup
import feedparser
import time
import sys
from typing import Optional, Dict, Any, List

class TechRadarScraper:
    def __init__(self, 
                 db_name: str = 'db/news.db',
                 feed_urls: List[str] = None):
        if feed_urls is None:
            self.feed_urls = [
                "https://www.techradar.com/feeds/tag/software",
                "https://www.techradar.com/feeds/tag/computing",
                "https://www.techradar.com/feeds/articletype/news"
            ]
        else:
            self.feed_urls = feed_urls

        self.db_name = db_name
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': ('Mozilla/5.0 (Windows NT 10.0; Win64; x64) '
                           'AppleWebKit/537.36 (KHTML, like Gecko) '
                           'Chrome/91.0.4472.124 Safari/537.36')
        })
        self.setup_database()

    def setup_database(self):
        try:
            with sqlite3.connect(self.db_name) as conn:
                c = conn.cursor()
                c.execute("""
                    CREATE TABLE IF NOT EXISTS articles (
                        link TEXT PRIMARY KEY,
                        title TEXT NOT NULL,
                        published_date TIMESTAMP,
                        content TEXT NOT NULL,
                        source TEXT NOT NULL,
                        processed_date TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                    )
                """)
                conn.commit()
        except sqlite3.Error as e:
            sys.exit(f"Database initialization error: {e}")

    def fetch_feed_entries(self) -> List[Dict[str, Any]]:
        all_entries = []
        seen_links = set()
        for feed_url in self.feed_urls:
            try:
                print(f"Fetching feed: {feed_url}")
                response = self.session.get(feed_url, timeout=10)
                response.raise_for_status()
                feed = feedparser.parse(response.text)
                for entry in feed.entries:
                    if entry.link not in seen_links:
                        seen_links.add(entry.link)
                        content = ''
                        if hasattr(entry, 'dc_content'):
                            content = entry.dc_content
                        elif hasattr(entry, 'content'):
                            content = entry.content[0].value if entry.content else ''
                        elif hasattr(entry, 'description'):
                            content = entry.description

                        all_entries.append({
                            'link': entry.link,
                            'title': entry.title,
                            'published_date': getattr(entry, 'published', None),
                            'content': self.clean_html_content(content)
                        })
            except Exception as e:
                print(f"Error fetching feed {feed_url}: {e}")
        return all_entries

    def clean_html_content(self, html_content: str) -> str:
        if not html_content:
            return ""
        soup = BeautifulSoup(html_content, 'html.parser')
        text = soup.get_text(separator=' ')
        return ' '.join(text.split())

    def scrape_article(self, url: str) -> Optional[str]:
        try:
            response = self.session.get(url, timeout=10)
            response.raise_for_status()
            soup = BeautifulSoup(response.content, 'html.parser')
            article_body = soup.find('div', {'id': 'article-body'})
            if not article_body:
                return None

            you_might_like = article_body.find('h3', string=lambda x: x and 'You might also like' in x)
            if you_might_like:
                current = you_might_like
                while current:
                    next_el = current.next_sibling
                    current.decompose()
                    current = next_el

            for unwanted in article_body.find_all(['div'], class_=['hawk-widget-insert', 'see-more', 'van_vid_carousel']):
                unwanted.decompose()

            content_elements = article_body.find_all(['p', 'h2', 'h3'])
            cleaned_paragraphs = []
            for element in content_elements:
                cleaned_text = self.clean_html_content(str(element))
                if cleaned_text.strip():
                    cleaned_paragraphs.append(cleaned_text)
            return '\n\n'.join(cleaned_paragraphs)
        except requests.RequestException as e:
            print(f"Request error while scraping {url}: {e}")
            return None
        except Exception as e:
            print(f"Error processing {url}: {e}")
            return None

    def already_processed(self, link: str) -> bool:
        try:
            with sqlite3.connect(self.db_name) as conn:
                c = conn.cursor()
                c.execute("SELECT link FROM articles WHERE link = ?", (link,))
                row = c.fetchone()
                return bool(row)
        except sqlite3.Error:
            return False

    def process_articles(self, limit: int = 100):
        feed_entries = self.fetch_feed_entries()
        if not feed_entries:
            print("No feed entries found.")
            return

        new_entries = []
        for entry in feed_entries:
            if not self.already_processed(entry['link']):
                new_entries.append(entry)
            if len(new_entries) >= limit:
                break

        if not new_entries:
            print("No new articles to process.")
            return

        for entry in new_entries:
            print(f"\nProcessing article: {entry['title']}")
            content = entry['content'] or self.scrape_article(entry['link'])
            if not content:
                print(f"Failed to get content for {entry['link']}\n")
                continue

            try:
                with sqlite3.connect(self.db_name) as conn:
                    c = conn.cursor()
                    c.execute("""
                        INSERT OR REPLACE INTO articles 
                        (link, title, published_date, content, source)
                        VALUES (?, ?, ?, ?, ?)
                    """, (
                        entry['link'],
                        entry['title'],
                        entry['published_date'],
                        content,
                        "techradar"
                    ))
                    conn.commit()
                
                print(f"Title: {entry['title']}")
                print(f"Link: {entry['link']}")
                print(f"Published: {entry['published_date']}")
                print("\nContent Preview:")
                print(content[:500] + "...\n")
                print("-" * 80 + "\n")

                time.sleep(2)
            except sqlite3.Error as db_error:
                print(f"Database error while storing article: {db_error}")
                continue

def main():
    feed_urls = [
        "https://www.techradar.com/feeds/tag/software",
        "https://www.techradar.com/feeds/tag/computing",
        "https://www.techradar.com/feeds/articletype/news"
    ]
    scraper = TechRadarScraper(feed_urls=feed_urls)
    scraper.process_articles(limit=100)

if __name__ == "__main__":
    main()

================
File: utils.py
================
"""
utils.py

Contains generic helper functions (hashing, token counting, chunking, etc.)
and the CVE regex from the original code.
"""

import re
import hashlib

MAX_TOKEN_CHUNK = 70000  # from the original script (~70k tokens)
CVE_REGEX = r'\bCVE-\d{4}-\d{4,7}\b'

def generate_content_hash(text):
    """Generate a simple MD5 hash for content."""
    return hashlib.md5(text.encode()).hexdigest()

def approximate_tokens(text: str) -> int:
    """Roughly estimate tokens by counting words and multiplying by ~1.3."""
    return int(len(text.split()) * 1.3)

def chunk_summaries(summaries_dict, max_token_chunk=MAX_TOKEN_CHUNK):
    """
    Splits article summaries into chunks without exceeding max_token_chunk.
    If a single article alone exceeds max_token_chunk, yield it alone.
    """
    current_chunk = {}
    current_tokens = 0

    for link, summary in summaries_dict.items():
        tokens_for_article = approximate_tokens(summary)
        if tokens_for_article > max_token_chunk:
            # If article alone exceeds chunk limit, yield current then yield it alone
            if current_chunk:
                yield current_chunk
                current_chunk = {}
                current_tokens = 0
            yield {link: summary}
            continue

        if current_tokens + tokens_for_article > max_token_chunk:
            if current_chunk:
                yield current_chunk
            current_chunk = {link: summary}
            current_tokens = tokens_for_article
        else:
            current_chunk[link] = summary
            current_tokens += tokens_for_article

    if current_chunk:
        yield current_chunk

def extract_cves(text: str):
    """Extract a set of unique CVE numbers from the provided text."""
    return set(re.findall(CVE_REGEX, text))



================================================================
End of Codebase
================================================================
